{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I devote considerable effort in this post to a theoretical enunciation of LSTM. A thorough derivation of the LSTM network is generally skipped in finance deep learning literature that features the LSTM. I seek to understand each aspect of the operation of RNN and LSTM's elegant and effective systems in considerable depth. I  describe firstly the RNN, followed by its evolution to the LSTM form, much more completely than is usually the case in finance DL papers, showing in detail the key operations I build in the empirical study in order to predict a financial time series.  These include unrolling the RNN, the motivations for the LSTM, namely managing the phenomena of vanishing gradients, and the subsequent mechanics of learning, which are the forward and backward passes and gradient descent.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Few papers in the literature appear to do this systematically and comprehensively, and the best treatment I have found is that of Sherstinsky (2018, 2020). As with much of the most useful empirical DL literature, Sherstinsky's papers are not finance papers and considerable work is required to transcribe the most relevant aspects of it to a finance setting. Sherstinsky (2020) uses a signal processing framework and notation from engineering and life sciences. He derives a canonical RNN set up and, within this framework, explains RNN unrolling. I show that a neural network at the simplest level of abstraction takes an input, $x_t$, log returns specifically in the case of this paper, and outputs a value, in practice represented in a normalized form. The loop (i.e. the 'recurrent' aspect) of the RNN allows information to be passed sequentially through steps of the neural network, from one step to the next, thus creating a structure for sequential learning.  I unroll a RNN, following the framework of Sherstinsky (2020), to reveal its chain-like nature and intimate relation to sequences. Viewed in this way, the RNN is an obvious and compelling candidate for time series analysis. I then proceed to describe fully the training routines, FP, BP and GD, along with the numerical phenomenon of vanishing gradients, first described by Schmidhuber's student, Hochreiter in his PhD thesis (1991), and subsequently addressed in the seminal LSTM paper of Hochreiter & Schmidhuber (1997). Sherstinsky provides a particularly complete enunciation of FP, BP, and GD, which I follow.  However, his overall framework is not particularly helpful from a finance perspective and considerable work is required to unpick it so that it readily ties back to the generally far briefer presentations that typify finance DL papers.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## LSTM in finance DL literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Finance researchers approaching DL, and the LSTM model specifically, appear rarely, if ever, to exposit the model fully.  An obvious way to do this is to start with the RNN, as, for example, Verstyuk (2020) and Sezer, Gudelek & Ozbayoglu (2019) do, but they are exceptions. The RNN equation is generally expressed in broader (i.e. non-finance) DL literature via input, $x$, and weight, $w$, terms, using a form that looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    h_t &= W f \\left( h_{t-1} \\right) + W ^{(hx)} x_t \\label{5.1} \\tag{5.1} \\\\ \\\\\n",
    "    y_t &= W^S f \\left( h_t \\right) \\label{5.2} \\tag{5.2}\n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $W$ denotes weight matrices and $h_t$ is an output value that in practice must be represented in a normalized form, $y_t$. Equation $\\ref{5.1}$ thus describes an output of a single RNN neuron, or node, whilst equation $\\ref{5.2}$ describes a nonlinear activation function that provides the cumulative output of all preceding neurons. The activation function thus transforms summed weighted input from the neuron into the activation of the neuron, or equivalently its output for that input.  \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "But it is more common for finance DL  papers addressing the LSTM to skip its origins and relation to the canonical RNN and proceed directly to a description of a system of equations, usually comprising 5 equations, that are held out to fully define a LSTM neuron, or cell as it is more commonly called in the case of the LSTM RNN. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The LSTM specializes the general RNN by introducing state vectors and it permits an input sequence, for example a financial returns time-series, to condition upon recent temporal dynamics. Its input variable, $x_t$, is in fact a lagged prediction of the output, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    x_t := y_{t-1}\n",
    "    \\label{5.3} \\tag{5.3}\n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "And the LSTM is generally represented as complete in a form resembling that of Verstyuk (2020) or Sezer, Gudelek & Ozbayoglu (2019):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    f_t &= \\mathrm{tanh} \\left( b_f + W_f x_t + U_f h_{t-1} \\right)  \\label{5.4} \\tag{5.4} \\\\ \\\\\n",
    "    o_t &= \\mathrm{tanh} \\left( b_o + W_o x_t + U_o h_{t-1} \\right)  \\label{5.5} \\tag{5.5} \\\\ \\\\\n",
    "    i_t &= \\mathrm{tanh} \\left( b_i + W_i x_t + U_i h_{t-1} \\right)  \\label{5.6} \\tag{5.6} \\\\ \\\\\n",
    "    c_t &= f_t \\ast c_{t-1} + i_t \\ast \\mathrm{tanh} \\left( b_c + W_c x_t + U_c h_{t-1} \\right)  \\label{5.7} \\tag{5.7} \\\\ \\\\\n",
    "    h_t &= o_t \\ast \\mathrm{tanh} \\left( c_t \\right)  \\label{5.8} \\tag{5.8}\n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where:\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$x_t$ denotes the LSTM unit, or cell's, input vector.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$f_t$ denotes the activation vector of a 'forget' gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$o_t$ denotes the activation vector of an 'output' gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$i_t$ denotes the activation function of an 'input' gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$c_t$ denotes a vector that represents the LSTM cell's state.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W, U$ denote weight matrices to be trained or learned.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$b$ denotes bias vector parameters to be trained or learned.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "However, I find expository discourse of the LSTM RNN in finance DL studies (see, for example, Verstyuk (2020), Petnehazi & Gall (2019), Nguyen, Tran, Gunawan & Kohn (2019), Kim & Kim (2019), Bucci (2019), Borovkova & Tsiamas (2019), Almosova & Andresen (2019), Ahmed, Smola, Wang, Xing, Zaheer & Zheng (2018), Namin & Namin (2018), Fischer & Krauss (2017), Xiong, Nichols & Shen (2016)) to be unsatisfying, not least because in execution it bears very little relation to the code one writes and uses to confront a LSTM RNN to a returns series. Theory is hardly pseudo-code, but the gap between what is presented in the methodology and what actually needs to be done in the empirical study is just sometimes too big in finance DL literature and too much generally seems to have been skipped or omitted. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I therefore wish to enunciate the RNN and its evolution to the LSTM form much more completely, showing in detail the key operations I build in order to predict a financial time series, such as unrolling a RNN, the motivations for the LSTM, namely managing the phenomena of vanishing gradients, and the subsequent mechanics of learning, namely the forward and backward passes and gradient descent.  Few papers in the literature appear to do this systematically and comprehensively, and the best treatment I have found is that of Sherstinsky (2018, 2020). As with much of the most useful empirical DL literature, Sherstinsky's papers are not finance papers and considerable work is required to transcribe the most relevant aspects of it to a finance setting. Sherstinsky (2020) nevertheless provides the most thorough exposition of the necessary RNN and LSTM operations, using a signal processing framework. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sherstinsky derives a canonical RNN set up using notation from engineering and life sciences and, within this framework, explains RNN unrolling. I was unable to find any reference to the process of unrolling a RNN in the finance-specific DL literature.  Generally papers move either directly from a stark presentation of the canonical RNN equation to an equally stark formulation of the LSTM system, as shown in equations $\\ref{5.1}$ to $\\ref{5.8}$, or, more often, directly to the latter, i.e. equations $\\ref{5.4}$ to $\\ref{5.8}$ alone.  This is unfortunate, not least because it misses an important reason why the RNN, and in turn the LSTM, may be of interest to researchers of financial time series and volatility forecasting. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As I show in figures 1 and 2, a neural network at the simplest level of abstraction takes an input, $x_t$, log returns specifically in the case of this paper, and outputs a value, $h_t$, which, as noted above, in practice is represented in a normalized form, $y_t$. The loop of the RNN simply allows information to be passed sequentially through steps of the neural network, from one step to the next, thus creating a structure for sequential learning.  By unrolling a RNN (figure 2), I reveal its chain-like nature and intimate relation to sequences. Viewed in this way, the RNN is an obvious and compelling candidate for time series analysis. Sherstinsky unrolls the RNN, then proceeds to describe fully the training routines, FP, BP and GD, along with the numerical phenomenon of vanishing gradients, first described by Schmidhuber's student, Hochreiter in his PhD thesis (1991), and subsequently addressed in the seminal LSTM paper of Hochreiter & Schmidhuber (1997). Sherstinsky provides a particularly complete enunciation of FP, BP, and GD.  However, his overall framework is not particularly helpful from a finance perspective and considerable work is required to unpick it so that it readily ties back to the far briefer presentations that typify finance DL papers.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "My $\\mathrm{WMP}_t$ series, defined as the natural log of the weighted mid-price of the AAPL, JPM securities or the EURUSD exchange rate, follows the construction in equations $\\ref{4.1}$ and $\\ref{4.2}$, and I define the return innovations $x_t \\equiv \\mathrm{WMP}_t - \\mathrm{WMP}_{t-1}$. I may write $x \\left( t \\right)$ in the form of an ordinary differential equation thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\frac{dh \\left( t \\right)} {dt} = g \\left( t \\right) + \\epsilon\n",
    "    \\label{5.9} \\tag{5.9}\n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $h_t$ is again an output value that in practice must be represented in a normalized form, $y_t$, following equations $\\ref{5.1}$ and $\\ref{5.1}$, $g \\left( t \\right)$ is a vector function of time, and $\\epsilon$ is a constant bias, or prediction error, term.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I follow, structurally, Sherstinsky (2020), but re-align many concepts and notation with broader finance DL literature (for example, to fit more closely with equations $\\ref{5.1}$ to $\\ref{5.8}$ and the approaches taken by Verstyuk (2020), Petnehazi & Gall (2019), Nguyen, Tran, Gunawan & Kohn (2019), Kim & Kim (2019), Bucci (2019), Borovkova & Tsiamas (2019), Almosova & Andresen (2019), Ahmed, Smola, Wang, Xing, Zaheer & Zheng (2018), Namin & Namin (2018), Fischer & Krauss (2017), Xiong, Nichols & Shen (2016), despite the considerable discrepancy in approach across these studies).  Sherstinsky invokes a canonical form for $g \\left( t \\right)$ whereby it is written in the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    g \\left( t \\right) = j \\left( h \\left( t \\right), x \\left( t \\right) \\right)\n",
    "    \\label{5.10} \\tag{5.10}\n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $x \\left( t \\right)$ is an input vector. Combining $\\ref{5.9}$ and $\\ref{5.10}$ yields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\frac{dh \\left( t \\right)} {dt} = j \\left( h \\left( t \\right), x \\left( t \\right) \\right) + \\epsilon\n",
    "    \\label{5.11} \\tag{5.11}\n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Following equation $\\ref{5.10}$, a special case of $g \\left( t \\right)$ employed in ML, may be written thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    g \\left( t \\right) = a \\left( t \\right) + b \\left( t \\right) + c \\left( t \\right)\n",
    "    \\label{5.12} \\tag{5.12}\n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $a \\left( t \\right)$, $b \\left( t \\right)$, and $c \\left( t \\right)$ are terms that collectively constitute what is generally denoted the 'additive model' (Hastie, Tibshirani & Friedman (2009) describe the generalized additive model in Chapter 9).  In ML and DL the additive model adds terms that determine rates of change of neuronal activities (Grossberg 1988, 2013).\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sherstinsky defines a saturating additive model with the three constituent terms, $a_t$, $b_t$, and $c_t$, which I redefine as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    a_t &= \\sum^{K_h - 1}_{k = 0} a_k \\left( h \\left( t - \\tau_k \\right) \\right) \\label{5.13} \\tag{5.13} \\\\ \\\\\n",
    "    b_t &= \\sum^{K_y - 1}_{k = 0} b_k \\left( y \\left( t - \\tau_k \\right) \\right) \\label{5.14} \\tag{5.14} \\\\ \\\\\n",
    "    y \\left( t - \\tau_y \\right) &= J \\left( h \\left( t - \\tau_r \\right) \\right) \\label{5.15} \\tag{5.15} \\\\ \\\\\n",
    "    c_t &= \\sum^{K_x - 1}_{k = 0} c_k \\left( x \\left( t - \\tau_k \\right) \\right) \\label{5.16} \\tag{5.16}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $y \\left( t \\right)$ is an output vector that represents $h \\left( t \\right)$ in a normalized form. In practice, $J \\left( \\cdot \\right)$, may be obtained with the hyperbolic tangent, $\\left( \\mathrm{tanh} \\right)$. Substituting equations $\\ref{5.13}$, $\\ref{5.14}$, $\\ref{5.15}$, and $\\ref{5.16}$, into equation $\\ref{5.12}$, then inserting into equation $\\ref{5.9}$, yields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac{d} {dt} h \\left( t \\right) &= \\sum^{K_h - 1}_{k = 0} a_k \\left( h \\left( t - \\tau_k \\right) \\right) \n",
    "    + \\sum^{K_y - 1}_{k = 0} b_k \\left( y \\left( t - \\tau_k \\right) \\right) \\\\\n",
    "    &+ \\sum^{K_x - 1}_{k = 0} c_k \\left( x \\left( t - \\tau_k \\right) \\right) + \\epsilon \\label{5.17} \\tag{5.17} \\\\ \\\\\n",
    "    y \\left( t - \\tau_y \\right) &= J \\left( h \\left( t - \\tau_r \\right) \\right)  \\label{5.18} \\tag{5.18} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Equation $\\ref{5.17}$ above is a type of nonlinear ordinary delay differential equation (DDE). It has discrete delays and its derivative at any given time may be obtained in terms of the values of it at previous times. The first component of Equation $\\ref{5.17}$, $\\sum^{K_h - 1}_{k = 0} a_k \\left( h \\left( t - \\tau_k \\right) \\right)$, is the output function. The second component, $\\sum^{K_y - 1}_{k = 0} b_k \\left( y \\left( t - \\tau_k \\right) \\right)$, is the normalized output function. And the third component, $\\sum^{K_x - 1}_{k = 0} c_k \\left( x \\left( t - \\tau_k \\right) \\right))$, is the input function.  The hyperbolic tangent, $\\mathrm{tanh}$, is generally used in practice as the normalization function.  The right hand side of equation $\\ref{5.17}$ consists of time delay terms that in turn comprise the system's memory. Collectively these terms enable the respective contributions of the output, normalized output, and input.  Time delay in NNs arises from the interaction between neurons (see for example Kyrychko & Hogan (2010)) and is thus an natural part of the system and its dynamics. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Each term on the right hand side of equation $\\ref{5.17}$ has a qualitatively different impact. $a_k \\left( h \\left( t - \\tau_k \\right) \\right)$ has a stability impact, $b_k \\left( y \\left( t - \\tau_k \\right) \\right)$, the normalized output vector, captures much of the information that determines long-term behavior. Explicit non-zero delay time constants account for the delays inherent in neural interactions (see for example de Vries & Principe (1991)). $K_h$, $K_y$, and $K_x$ thus represent counts of the functions $a_k \\left( h \\left( t - \\tau_k \\right) \\right)$, $b_k \\left( y \\left( t - \\tau_k \\right) \\right)$, and $c_k \\left( x \\left( t - \\tau_k \\right) \\right)$ respectively, along with the associated counts of $\\tau_h \\left( k \\right )$, $\\tau_y \\left( k \\right )$, and $\\tau_x \\left( k \\right )$ respectively. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let $a_k \\left( h \\left( t - \\tau_k \\right) \\right)$, $b_k \\left( y \\left( t - \\tau_k \\right) \\right)$, and $c_k \\left( x \\left( t - \\tau_k \\right) \\right)$ be linear functions of $h$, $y$, and $x$ respectively. Then I may re-write equation $\\ref{5.17}$ with linear (matrix-valued) coefficients, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac{d} {dt} h \\left( t \\right) &= \\sum^{K_h - 1}_{k = 0} A_k \\left( h \\left( t - \\tau_k \\right) \\right) \n",
    "    + \\sum^{K_y - 1}_{k = 0} B_k \\left( y \\left( t - \\tau_k \\right) \\right) \\\\\n",
    "    &+ \\sum^{K_x - 1}_{k = 0} C_k \\left( x \\left( t - \\tau_k \\right) \\right) + \\epsilon \\label{5.19} \\tag{5.19} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I apply the following simplification to equation $\\ref{5.19}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    K_h &= 1 \\\\\n",
    "    \\tau_{h, 0} &= 0 \\\\\n",
    "    A_0 &= A \\\\\n",
    "    K_y &= 1 \\\\\n",
    "    \\tau_{y, 0} &= \\tau_0 \\\\\n",
    "    B_0 &= B \\\\\n",
    "    K_x &= 1 \\\\\n",
    "    \\tau_{x, 0} &= 0 \\\\\n",
    "    C_0 &= C \n",
    "    \\label{5.20} \\tag{5.20}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In order to re-write equation $\\ref{5.19}$ thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\frac{d} {dt} h \\left( t \\right) = A h \\left( t \\right) + B y \\left( t - \\tau_0\\right) + C x \\left( t \\right) + \\epsilon\n",
    "    \\label{5.21} \\tag{5.21}\n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Equation $\\ref{5.19}$ and $\\ref{5.21}$ are non-linear first order DDE. Sherstinsky (2020) uses numerical integration to evaluate these equations, applying the backward Euler discretization rule (following Butcher, 2003).\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I denote a sampling time step duration $\\Delta T$ and index time, $n$. I then apply the backward Euler method in the manner of Sherstinsky (2020) to equation $\\ref{5.21}$, yielding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    t &= n \\Delta T \\label{5.22} \\tag{5.22} \\\\ \\\\\n",
    "    \\frac{d} {dt} h \\left( t \\right) &\\sim \\frac {h \\left( n \\Delta T + \\Delta T \\right) - h \\left( n \\Delta T \\right)} {\\Delta T} \\label{5.23} \\tag{5.23} \\\\ \\\\\n",
    "    A h \\left( t \\right) + B y \\left( t - \\tau_0 \\right) + C x \\left( t \\right) + \\epsilon &= A h \\left( n \\Delta T \\right) + B y \\left( n \\Delta T  - \\tau_0 \\right) \\\\\n",
    "    &+ C x \\left ( n \\Delta T \\right) + \\epsilon \\label{5.24} \\tag{5.24} \\\\ \\\\\n",
    "    A h \\left( t + \\Delta T \\right) + B y \\left( t + \\Delta T - \\tau_0 \\right) + C x \\left( t + \\Delta T \\right) + \\epsilon &= A h \\left( n \\Delta T + \\Delta T \\right) + B y \\left( n \\Delta T  + \\Delta T - \\tau_0 \\right) \\\\ \n",
    "    &+ C x \\left ( n \\Delta T + \\Delta T \\right) + \\epsilon \\label{5.25} \\tag{5.25} \\\\ \\\\\n",
    "    \\frac {h \\left( n \\Delta T + \\Delta T \\right) - h \\left( n \\Delta T \\right)} {\\Delta T} &\\sim A h \\left( n \\Delta T + \\Delta T \\right) + B y \\left( n \\Delta T  + \\Delta T - \\tau_0 \\right) \\\\\n",
    "    &+ C x \\left ( n \\Delta T + \\Delta T \\right) + \\epsilon \\label{5.26} \\tag{5.26}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The delay, $\\tau_0$, may be set equal to a single time step, so that the output and normalized output is read into memory at each time step and used in the above equations for the next time step as an ongoing recursion, for the time step duration, $T$.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Setting $\\tau_0 = \\Delta T$ in equation $\\ref{5.26}$ thus yields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac {h \\left( n \\Delta T + \\Delta T \\right) - h \\left( n \\Delta T \\right)} {\\Delta T} &= A h \\left( n \\Delta T + \\Delta T \\right) + B y \\left( n \\Delta T \\right) + C x \\left ( n \\Delta T + \\Delta T \\right) + \\epsilon \\label{5.27} \\tag{5.27} \\\\ \\\\\n",
    "    \\frac {h \\left( \\left( n + 1 \\right) \\Delta T \\right) - h \\left( n \\Delta T \\right)} {\\Delta T} &= A h \\left( \\left( n + 1 \\right) \\Delta T \\right) + B y \\left( n \\Delta T \\right) + C x \\left( \\left( n + 1 \\right) \\Delta T \\right) + \\epsilon \\label{5.28} \\tag{5.28} \\\\ \\\\\n",
    "    h \\left( \\left( n + 1 \\right) \\Delta T \\right) - h \\left( n \\Delta T \\right) &= \\Delta T \\left( A h \\left( \\left( n + 1 \\right) \\Delta T \\right) + B y \\left( n \\Delta T \\right) + C x \\left( \\left( n + 1 \\right) \\Delta T \\right) + \\epsilon \\right) \\label{5.29} \\tag{5.29} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As a result of the discretization in equation $\\ref{5.29}$, measures of time become integral multiples of $\\Delta T$. Omitting $\\Delta T$ renders the time axis dimensionless and transforms the vectors into sequences and thus equation $\\ref{5.21}$ into a non-linear first order difference equation, which I write thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_{t+1} - h_t &= \\Delta T \\left( A h_{t+1} + B y_t + C x_{t+1} + \\epsilon \\right ) \\label{5.30} \\tag{5.30} \\\\ \\\\\n",
    "    h_{t+1} &= h_t + \\Delta T \\left( A h_{t+1} + B y_t + C x_{t+1} + \\epsilon \\right ) \\\\ \\\\\n",
    "    \\left( I - \\left( \\Delta T \\right) A \\right) h_{t+1} &= h_t + \\left( \\left( \\Delta T \\right) B \\right) y_t + \\left( \\left( \\Delta T \\right) C \\right) x_{t+1} + \\left( \\Delta T \\right) \\epsilon \\label{5.31} \\tag{5.31} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I define a weight matrix, $W_h$, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    W_h = {\\left( I - \\left( \\Delta T \\right) A \\right)}^{-1} \\label{5.32} \\tag{5.32} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Multiply both sides of equation $\\ref{5.31}$ by $W_h$, thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_{t+1} = W_h h_t\n",
    "    + \\left( \\left( \\Delta T \\right) W_h B \\right) y_t \n",
    "    + \\left( \\left( \\Delta T \\right) W_h C \\right) x_{t+1} \n",
    "    + \\left( \\left( \\Delta T \\right) W_h \\epsilon \\right)\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "And move forward 1 step, thus yielding: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_t &= W_h h_{t-1} + \\left( \\left( \\Delta T \\right) W_h B \\right) y_{t-1} + \\left( \\left( \\Delta T \\right) W_h C \\right) x_t + \\left( \\left( \\Delta T \\right) W_h \\epsilon \\right) \\\\ \\\\\n",
    "    y_t &= J \\left( h_t \\right) \\label{5.33} \\tag{5.33}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I define two additional weight matrices, $W_y$ and $W_x$, along with a bias vector, $b_h$, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    W_y &= \\left( \\Delta T \\right) W_h B \\label{5.34} \\tag{5.34} \\\\ \\\\\n",
    "    W_x &= \\left( \\Delta T \\right) W_h C \\label{5.35} \\tag{5.35} \\\\ \\\\\n",
    "    b_h &= \\left( \\Delta T \\right) W_h \\epsilon \\label{5.36} \\tag{5.36}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Finally, I express the above system in the canonical RNN form, following Sherstinsky (2020), Kornblith, Norouzi, Lee & Hinton (2019), and Schmidhuber (2015, 2013):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_t &= W_h h_{t-1} + W_y y_{t-1} + W_x x_t + b_h \\label{5.37} \\tag{5.37} \\\\ \\\\\n",
    "    y_t &= J \\left( h_t \\right) \\label{5.38} \\tag{5.38}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Figure 1 provides an illustration of the canonical RNN formulation (equation $\\ref{5.37}$). \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Figure 1. Canonical RNN cell](fig_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Equation $\\ref{5.37}$ is stable if the eigenvalues of $\\hat W = W_h + W_y$ are within the complex-valued unit circle (i.e., the complex number set that has a magnitude of one; see Sherstinsky, 2018, 2020). $A$ and $B$ has a very many elements potentially that satisfy this requirement, so that $\\Delta T = 1$ may be set, for tractability.  As a result, $h_{t-1}$ has negligible impact and may be ignored, reducing equation $\\ref{5.37}$ to a standard RNN definition (recall, for example, equation $\\ref{3.3}$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_t &= W_y y_{t-1} + W_x x_{t} + b_h \\label{5.39} \\tag{5.39} \\\\ \\\\\n",
    "    y_t &= J \\left( h_t \\right) \\label{5.40} \\tag{5.40}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "By adapting the framework of Sherstinsky (2020) to complement and considerably extend finance DL literature, I have shown that the RNN, in canonical form (equation $\\ref{5.37}$) or equation $\\ref{5.39}$, essentially implements the backward Euler method for the ordinary DDE $\\ref{5.21}$. This 'forward' direction of starting in the continuous time domain (differential equation) and ending in the discrete domain (difference equation) implies that the phenomenon modeled by the econometrician is analog, and that it is modeled for purposes of realization in the discrete domain. For example, the source signal might be AAPL's price on the NYSE, as recorded on the NYSE TAQ. The orginal signal thus contains all information regarding the price of AAPL. The samples generated by a hypothetical discretization, governed in this model by equation $\\ref{5.21}$, could be captured as sparse samples , of the kind, for example, used in the empirical study (see sections 4 and 6). In this scenario, it is the sparsely sampled WMP that the RNN reproduces, not the actual price of AAPL. The key, if somewhat subtle, point in this scenario is that applying the RNN as a model implies that the price of AAPL is governed by equation $\\ref{5.21}$, so that the role of the RNN is that of implementing the computational method for solving this delay differential equation using the backward Euler discretization rule, under the restriction that the sampling time step, $\\Delta T$, is equal to the delay, $\\tau_0$. In contrast, the 'reverse' direction might be a more appropriate model in situations where the discrete signal is the natural starting domain, for example a situation where I am only able to obtain a dataset comprising, say, 1-minute, or 5-minute, or 10-minute samples of AAPL's price. While the starting point is data-dependent, both the continuous ('forward') and discrete ('reverse') representations may provide insight into the advantages and limitations of the models under consideration. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Unrolling the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In the manner of broader neural network literature (see for example Goodfellow, Bengio & Courville (2016) and Grossberg (2013)), I use the term \"cell\" with reference to equations $\\ref{5.37}$and/ or $\\ref{5.39}$.  The cell is uninitialized at this point, meaning the sequence has been defined but not yet computed. The process of unrolling the RNN is that of specifying initial conditions for $h_t$ followed by numerical evaluation of $\\ref{5.37}$, or $\\ref{5.39}$, for the number of steps in the sequence. I have illustrated this process in figure 2:\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Figure 2. Unrolling a RNN cell](fig_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Both equations $\\ref{5.37}$ and $\\ref{5.39}$ are recursive in the state signal $s \\left[ n \\right]$. Thus, as a result of repeated application of the recurrence relation as part of RNN unrolling, the state signal, $s \\left[ n \\right]$, at some value of the index, $n$, encompasses the contributions of the state signal $s \\left[ k \\right]$, and the input signal, $x \\left[ k \\right]$, for all indices, $k < n$, ending at $k = 0$, the start of the sequence.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I define a step function, $g \\left( x \\right)$, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{equation*}\n",
    "    g \\left( x \\right) =\n",
    "    \\begin{cases}\n",
    "    1,  & x \\geq 0 \\\\\n",
    "    0,  & x < 0\n",
    "    \\end{cases} \\label{5.41} \\tag{5.41}\n",
    "\\end{equation*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $0$ and $1$ are vectors comprising only elements equal to $0$ and $1$ respectively. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I define a unit delta function, $\\delta \\left( x \\right)$ thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\delta \\left( x \\right) = g \\left( x \\right) - u \\left( x - 1 \\right)\n",
    "    \\label{5.42} \\tag{5.42} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $\\delta \\left( x \\right)$ is defined by being $1$ at $x = 0$ and $0$ otherwise.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I use the unit step and unit delta functions to simulate a sequence of steps and compute the response of $h_t$, to see if a pattern emerges. For equation $\\ref {5.37}$, with $b_h = 0$, the sequence for $h_t$ may be written in this way thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_{t-1} &= 0 \\\\\n",
    "    h_t &= W_x 1 \\\\\n",
    "    h_{t+1} &= W_y J \\left( W_x 1 \\right) \\\\\n",
    "    h_{t+2} &= W_y J \\left( W_y J \\left( W_x 1 \\right) \\right) \\\\\n",
    "    h_{t+3} &= W_y J \\left( W_y J \\left( W_y J \\left( W_x 1 \\right) \\right) \\right) \\\\   \n",
    "    h_{t+4} &= W_y J \\left( W_y J \\left( W_y J \\left( W_y J \\left( W_x 1 \\right) \\right) \\right) \\right) \\\\    \n",
    "    \\cdots\n",
    "    \\label{5.43} \\tag{5.43}   \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "'Ground truth' is a term used in various fields of study, usually to describe empirical evidence, as opposed to information provided by inference. In deep learning, ground truth generally refers to the accuracy of a training dataset's classification for SL techniques (see for example Schmidhuber 2015, 2013) and thus may be used for, say, hypotheses testing. Ground truth values to the econometrician who writes her own code is an array of 'labels'. I denote a sequence of ground truth output values or labels, $l$, and let $t$ denote the number of time steps in the sequence, $l \\left( T \\right)$, where each time step equates to an observation, for example a tick-by-tick price observation in my HF datasets, and $T$ can be an arbitrarily large integer (the total number of samples in the HF training dataset, for example).\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I subdivide $l \\left( T \\right)$, into $N$ discrete segments comprising $k_n$ samples per segment, thus:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    l \\left(T \\right) = \\sum^{N - 1}_{n = 0} l_n \\left( t \\right)\n",
    "    \\label{5.44} \\tag{5.44} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The process of sub-dividing ground truth into $M$ non-overlapping segments is, in practice, often called 'windowing'. I therefore define a 'rectangular' window function, $w \\left( x \\right)$, taking the value $1$ inside the window and $0$ otherwise. In terms of the unit step function, $g \\left( x \\right)$ (equation $\\ref{5.41}$), $w \\left( x \\right)$ may be defined thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    w \\left( x \\right) = g \\left( x \\right) - g \\left( x - k_0 \\right)\n",
    "    \\label{5.45} \\tag{5.45} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I may obtain an alternative 'sampling' definition or  $w_0 \\left( x \\right)$ by combining equations $\\ref{5.42}$ and $\\ref{5.45}$ thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    w_0 \\left( x \\right) &= l_x - l_{x-1} \\\\\n",
    "    &+ l_{x-1} - l_{x-2} \\\\\n",
    "    &+ l_{x-2} - l_{x-3} \\\\\n",
    "    &+ ... + \\\\\n",
    "    &+ l_{x-{k_0-2}} - l_{x-{k_0-1}}  \\\\\n",
    "    &+ l_{x-{k_0-1}} - l_{x-{k_0}}  \\\\\n",
    "    &= \\sum^{K_0 - 1}_{k = 0} \\delta \\left[ x - k \\right] \n",
    "    \\label{5.46} \\tag{5.46}   \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In this way, from $\\ref{5.46}$, I may 'window' the RNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    w \\left( x \\right) &= l_x - l_{x-{k_0}} \\\\ \\\\\n",
    "    &= \\sum^{M - 1}_{m = 0} \\left[ \\sum^{z \\left( m \\right) + K_m - 1}_{k = z \\left( m \\right)} \\delta \\left( x - k \\right) \\right] \\label{5.47} \\tag{5.47}  \\\\ \\\\\n",
    "    &= \\sum^{M - 1}_{m = 0} w_m \\left( x \\right) \\label{5.48} \\tag{5.48}   \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{equation*}\n",
    "    z \\left( m \\right) =\n",
    "    \\begin{cases}\n",
    "    \\sum^{m-1}_{i=0} K_i,  & 1 \\leq m \\leq M - 1 \\\\\n",
    "    0,  & m = 0\n",
    "    \\end{cases} \\label{5.49} \\tag{5.49}\n",
    "\\end{equation*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "And:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    w_m \\left( x \\right) = \\sum^{z \\left( m \\right) + K_m - 1}_{k = z \\left( m \\right)} \\delta \\left( x - k \\right)\n",
    "    \\label{5.50} \\tag{5.50} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I make the following adjustments $v = k - z \\left( m \\right)$, $k = z \\left( m \\right) + l$, and $v \\longrightarrow k$, in order to re-write equation $\\ref{5.50}$ thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    w_m \\left( x \\right) = \\sum^{K_m - 1}_{k = 0} \\delta \\left( x - z_m - k \\right)\n",
    "    \\label{5.51} \\tag{5.51} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Equation $\\ref{5.51}$ shows how each $w_m \\left( x \\right)$ comprises a a rectangular window of size $K_m$ samples. Thus, selecting a $K_m$-samples-long segment from ground truth equates to an element-wise multiplication of it by $w_m \\left( x \\right)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    l_m \\left( x \\right) &= w_m \\left( x \\right) \\ast v l \\left( x \\right) \\label{5.52} \\tag{5.52} \\\\ \\\\\n",
    "    &=\n",
    "    \\begin{cases}\n",
    "    l \\left( x \\right),  &z_m \\leq x \\leq z_m + k_m -1 \\\\\n",
    "    0,  &\\mathrm{otherwise} \n",
    "    \\end{cases} \\label{5.53} \\tag{5.53}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $z \\left( m \\right)$ is given by equation $\\ref{5.49}$. I define $\\mathcal{A} \\left( \\langle y \\left( x \\right) \\rangle \\right)$ as an invertible linear transformation thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\langle v \\left( x \\right) \\rangle = \\mathcal{A} \\left( \\langle y \\left( x \\right) \\rangle \\right)\n",
    "    \\label{5.54} \\tag{5.54} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "And I define $\\mathcal{L}$ as a loss or objective function (i.e. a measure of the 'cost' of $h_t$ and $y_t$ deviating from ground truth, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\mathcal{L} \\langle v \\left( x \\right) \\rangle  \\; , \\; \\langle l \\left( x \\right) \\rangle    \n",
    "    \\label{5.55} \\tag{5.55} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $\\langle v_x \\rangle$ denotes the sequence of the observable output variables, $v_x$, and $\\langle l_x \\rangle$ denotes the sequence of ground truth output values, $l_x$. I may now write all parameters of the standard RNN system (i.e. equation $\\ref{5.39}$, and recall again equation $\\ref{3.3}$) thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\Theta \\equiv \\left\\{ W_y, W_x, b_h \\right\\}  \n",
    "    \\label{5.56} \\tag{5.56} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Following Sherstinsky (2020), I now explain the assumptions that form the basis of RNN unrolling. Firstly, given $\\ref{5.39}$, parameterized by $\\Theta$ (equation $\\ref{5.56}$), I may assume a value of $\\Theta$ exists at which the objective function $\\mathcal{L}$ (equation $\\ref{5.55}$) is close to an optimum. Secondly, I may also assume non-zero constants, $M$ and $K_m$, such that $K_m < X$, where $0 \\leq m \\leq M - 1$, and that ground truth, $l_x$, may be windowed (i.e. equation $\\ref{5.53}$). In this way, I show that a single RNN cell, unrolled for $K_m$ steps, is computationally sufficient to obtain a value of $\\Theta$ that optimizes $\\mathcal{L}$ over a training dataset.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The loss or objective function in equation $\\ref{5.55}$ computes the error in the system's performance during training, validation, and testing phases and also tracks its generalization metrics with regard to actual application data during the inference phase (see section 6.1 Implementation). I state an assumption above that $\\mathcal{L}$ may be optimized.  Thus, by implication, when $\\mathcal{L}$ approaches an optimum, the observable output from the RNN system approximates ground truth:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\langle v \\left( x \\right) \\rangle \\sim \\langle l \\left( x \\right) \\rangle  \n",
    "    \\label{5.57} \\tag{5.57} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I may now process the RNN's output using equation $\\ref{5.52}$ to yield:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    v_m \\left( x \\right) &= w_m \\left( x \\right) \\ast l \\left( x \\right) \\label{5.58} \\tag{5.58} \\\\ \\\\\n",
    "    &=\n",
    "    \\begin{cases}\n",
    "    y_x,  &z_m \\leq x \\leq z_m + k_m -1 \\\\\n",
    "    0,  &\\mathrm{otherwise} \n",
    "    \\end{cases} \\label{5.59} \\tag{5.59}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $z_m$ is given by equation $\\ref{5.49}$.  \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Following equation $\\ref{5.59}$, the individual output subsequences, $v_m \\left( x \\right)$ (equation $\\ref{5.58}$), shall be non-zero only when $x$ is in the range $z_m \\leq z \\leq j_m + k_m -1$. And following the assumption that ground truth values are mutually independent, the loss function in equation $\\ref{5.55}$ is separable.  The loss function may therefore be described via a set of $M$ independent segment-level components, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\mathcal{L} \\left( \\langle v_x \\rangle \\; , \\; \\langle l_x \\rangle  \\right) =\n",
    "    \\mathcal{C} \\left( \\mathcal{L} \\left[ \\langle v_m \\left( x \\right) \\rangle \\; , \\; \\langle l_m \\left( x \\right) \\rangle \\right] \\right)\n",
    "    \\label{5.60} \\tag{5.60} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $\\mathcal{C}$ acts simply as a suitable combining function. As a result of equations $\\ref{5.57}$ and $\\ref{5.60}$ I may now write:\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\langle v_m \\left( x \\right) \\rangle \\sim \\langle l_m \\left( x \\right) \\rangle    \n",
    "    \\label{5.61} \\tag{5.61} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Because $\\mathcal{A} \\left( \\langle y_x \\rangle \\right)$ is invertible, I may write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\langle y_m \\left( x \\right) \\rangle = \\mathcal{A}^{-1} \\left( \\langle v_m \\left( x \\right) \\rangle \\right)\n",
    "    \\label{5.62} \\tag{5.62} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Recall that the output normalization function, $J \\left( h_t \\right)$, (equation $\\ref{5.40}$) is invertible. I may therefore write for any $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    h_m \\left( x \\right) = J^{-1} \\left( \\langle v_m \\left( x \\right) \\rangle \\right)\n",
    "    \\label{5.63} \\tag{5.63} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Element-wise multiplication of the input sequence, $x_t$ and sampling window, $w_m \\left( x \\right)$, yields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    x_m \\left( x \\right) &= w_m \\left( x \\right) \\ast x_m \\left( x \\right) \\label{5.64} \\tag{5.64} \\\\ \\\\\n",
    "    &=\n",
    "    \\begin{cases}\n",
    "    x_m \\left( x \\right),  &z_m \\leq x \\leq z_m + k_m - 1 \\\\\n",
    "    0,  &\\mathrm{otherwise} \n",
    "    \\end{cases} \\label{5.65} \\tag{5.65}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I define $\\mathcal{G} \\left( \\langle x_m \\left( x \\right) \\rangle \\leq x \\leq z_m + k_m - 1 \\right)$ as a transformation of the input sequence, $x$, into the output value $h$, so that the standard RNN definition in equation $\\ref{5.39}$ for $k_m$ samples may be written thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align}\n",
    "    \\langle h_m \\left( x \\right) \\rangle \\leq x \\leq z_m + k_{m - 1} \n",
    "    = \\mathcal{G} \\left( \\langle x_m \\left( x \\right) \\rangle \\leq x \\leq z_m + k_m - 1 \\right)\n",
    "    \\label{5.66} \\tag{5.66} \n",
    "\\end{align}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I may now substitute equations $\\ref{5.65}$ and $\\ref{5.66}$ into equation $\\ref{5.39}$ to produce the RNN system equations for an individual window:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_m \\left( x \\right) &=\n",
    "    \\begin{cases}\n",
    "    W_y y_m \\left( x \\right) + W_x x_m \\left( x \\right) + b_h,  &z_m  \\leq x \\leq z_m + k_m - 1 \\\\\n",
    "    0,  &\\mathrm{otherwise}\n",
    "    \\end{cases} \\label{5.67} \\tag{5.67} \\\\ \\\\\n",
    "    y_m \\left( x \\right) &= \n",
    "    \\begin{cases}\n",
    "    J \\left( h_m \\left( x \\right) \\right),  &z_m  \\leq x \\leq z_m + k_m - 1 \\\\\n",
    "    0,  &\\mathrm{otherwise}\n",
    "    \\end{cases} \\label{5.68} \\tag{5.68} \\\\ \\\\\n",
    "    h_m \\left[ x = z_m - 1 \\right]  &= 0 \\label{5.69} \\tag{5.69} \\\\ \\\\\n",
    "    0 &\\leq m \\leq M - 1 \\label{5.70} \\tag{5.70}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I substitute $x \\longrightarrow x + j \\left( m \\right)$ to shift $y_m \\left( x \\right)$, $h_m \\left( x \\right)$, and $x_m \\left( x \\right)$, by $- z_m$ samples, thus:\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    x_m \\left( x \\right) &\\equiv x_m \\left( x + z_m \\right) \\\\ \\\\\n",
    "    &=\n",
    "    \\begin{cases}\n",
    "    x \\left( x + z_m \\right),  &z_m \\leq x + j_m \\leq j_m + k_m - 1 \\\\\n",
    "    0,  &\\mathrm{otherwise}\n",
    "    \\end{cases} \\\\ \\\\\n",
    "    &=\n",
    "    \\begin{cases}\n",
    "    x \\left( x + z_m \\right),  &0 \\leq x \\leq K_m - 1  \\\\\n",
    "    0,  &\\mathrm{otherwise}\n",
    "    \\end{cases} \\label{5.71} \\tag{5.71} \\\\ \\\\    \n",
    "    y_m \\left( x \\right) &\\equiv y_m \\left( x + z_m \\right) \\\\ \\\\\n",
    "    h_m \\left( x \\right) &\\equiv h_m \\left( x + z_m \\right) \\label{5.72} \\tag{5.72} \\\\ \\\\ \n",
    "    &=\n",
    "    \\begin{cases}\n",
    "    W_y y_m \\left( x + z_m - 1 \\right) + W_x x_m \\left( x + z_m \\right) + b_h,  &z_m \\leq x + z_m \\leq z_m + k_m - 1 \\\\\n",
    "    0,  &\\mathrm{otherwise}\n",
    "    \\end{cases} \\\\ \\\\\n",
    "    &=\n",
    "    \\begin{cases}\n",
    "    W_y y_m \\left( x \\right) + W_x x_m \\left( x \\right) + b_h,  &0 \\leq x \\leq K_m - 1  \\\\\n",
    "    0,  &\\mathrm{otherwise}\n",
    "    \\end{cases} \\label{5.73} \\tag{5.73} \\\\ \\\\    \n",
    "    h_m \\left( x - 1 \\right) &\\equiv h_m \\left[ x + z_m = z_m - 1 \\right] = 0 \\label{5.74} \\tag{5.74}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "And simplify thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_m \\left( x - 1 \\right)  &= 0 \\label{5.75} \\tag{5.75} \\\\ \\\\\n",
    "    h_m \\left( x \\right) &=\n",
    "    \\begin{cases}\n",
    "    W_y  y_m \\left( x - 1 \\right) + W_x x_m \\left( x \\right) + b_h,  &0 \\leq x \\leq K_m - 1 \\\\\n",
    "    0,  &\\mathrm{otherwise}\n",
    "    \\end{cases} \\label{5.76} \\tag{5.76} \\\\ \\\\\n",
    "    y_m \\left( x \\right) &= \n",
    "    \\begin{cases}\n",
    "    J \\left[ h_m \\left( x \\right) \\right],  &0 \\leq x \\leq K_m - 1 \\\\\n",
    "    0,  &\\mathrm{otherwise}\n",
    "    \\end{cases} \\label{5.77} \\tag{5.77} \\\\ \\\\\n",
    "    x_m \\left( x \\right) &= \n",
    "    \\begin{cases}\n",
    "    x \\left[ x + z_m \\right], &0 \\leq x \\leq K_m - 1 \\\\\n",
    "    0, &\\mathrm{otherwise}\n",
    "    \\end{cases} \\label{5.78} \\tag{5.78}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As per the standard RNN system, equation $\\ref{5.39}$, the input sequence $x_m \\left( x \\right)$, is the only independent variable, unrolled for $K_m$ steps in equation $\\ref{5.76}$. Along with the mutual independence of $h_m \\left( x \\right)$, this makes the RNN computationally generic for all windows.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I now remove subscript $m$ from $h$ and $y$ to yield a final formulation of the RNN, unrolled for $K_m$ steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h \\left( x = -1 \\right) &= 0 \\label{5.79} \\tag{5.79} \\\\ \\\\\n",
    "    h \\left[ n \\right] &=\n",
    "    \\begin{cases}\n",
    "    W_y y \\left( x - 1 \\right) + W_x x_m \\left( x \\right) + b_h,  &0 \\leq x \\leq K_m - 1 \\\\\n",
    "    0,  &\\mathrm{otherwise}\n",
    "    \\end{cases} \\label{5.80} \\tag{5.80} \\\\ \\\\\n",
    "    y \\left[ n \\right] &= \n",
    "    \\begin{cases}\n",
    "    J \\left( h_x \\right),  &0 \\leq x \\leq K_m - 1 \\\\\n",
    "    0,  &\\mathrm{otherwise}\n",
    "    \\end{cases} \\label{5.81} \\tag{5.81} \\\\ \\\\\n",
    "    x_m \\left( x \\right) &= \n",
    "    \\begin{cases}\n",
    "    x \\left( x + z_m \\right), &0 \\leq x \\leq K_m - 1 \\\\\n",
    "    0, &\\mathrm{otherwise} \n",
    "    \\end{cases} \\label{5.82} \\tag{5.82} \\\\ \\\\\n",
    "    0 &\\leq m \\leq M - 1 \\label{5.83} \\tag{5.83}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This formulation of the RNN, unrolled for $K_m$ steps, processes the windows, one at a time. Equation $\\ref{5.79}$ initializes $h$, the cell output. Equation $\\ref{5.82}$ then selects the input samples, following which equations $\\ref{5.80}$ and $\\ref{5.81}$ may be applied for $K_m$ steps, $0 \\leq x \\leq K_m - 1$. The same process may be applied recursively until all windows have been processed.  Mutual independence allows for computation of $h_m \\left[ n \\right]$, $y_m \\left[ n \\right]$, and $x_m \\left[ n \\right]$ to be carried out concurrently, as, for example, when using a GPU (see section 6.1 Implementation).\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In the preceding two sections, following the framework of Sherstinsky (2020), I have shown how equations $\\ref{5.79}$ thru $\\ref{5.83}$, together with equation $\\ref{5.49}$, fully describe an unrolled RNN, in turn specified by equations $\\ref{5.39}$ and $\\ref{5.40}$. I proceed now to the process of 'training' a RNN in order to obtain its weights, focusing upon equations $\\ref{5.80}$ and $\\ref{5.81}$. RNN systems suffer from so-called vanishing (or exploding) gradients. In section 3.1, Deep learning history and topography, I explained that Hochreiter (1991) originally showed that with standard activation functions, cumulative back-propagated error signals either shrank rapidly, or grew out of bounds. I also noted that much subsequent research, particularly in the 1990’s and 2000’s, was motivated by this insight (see for example Pascanu, Mikolov & Bengio (2013), Hochreiter, Bengio, Paolo & Schmidhuber (2001), Hochreiter & Schmidhuber (1997)). I explained in section 3.1 the historic context whereby truncated unrolled RNN systems, such as equation $\\ref{5.80}$, came to be  commonly trained using BP, adapted for sequences (see Werbos (1990, 1988) for detailed reviews). BP is quintessentially the repeated application of the chain rule of differentiation. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "BP uses $x_m \\left( x \\right)$ and $y \\left( x \\right)$ in the training dataset to compute the parameters of the system, $\\Theta$ (equation $\\ref{5.56}$), in order to optimize a loss function, $L$. If GD, or alternative gradient algorithms (see for example Goodfellow, Bengio & Courville (2016) for a review), are used to simulate an optimum for $L$, the elements of $\\frac {\\partial L} {\\partial \\Theta}$ may be obtained via BP using the chain rule. Once the infinite RNN sequence in equation $\\ref{5.39}$ is unrolled, the resulting system given in equation $\\ref{5.80}$ becomes inherently stable and thus I may assume that the objective function $L$ may in turn take on the same form for all segments. I therefore now apply BP to $\\ref{5.80}$. Consider that $L$ requires $y$ to compute the gradient of $L$ with respect to $y$, which I may call $\\chi$, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\chi \\left( x \\right) \\equiv \\nabla_{y_x} L = \\frac {\\partial L} {\\partial y_x}\n",
    "    \\label{5.84} \\tag{5.84}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$y_x$ relies upon $h_x$, thus $h_x$ may also influence $L$, so that the  gradient of $L$ with respect to $h_x$, which I denote $\\varphi$, ought be computed thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\varphi \\left( x \\right) \\equiv \\nabla_{h_x} L = \\frac {\\partial L} {\\partial h_x}\n",
    "    \\label{5.85} \\tag{5.85}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$\\chi$ and $\\varphi$ in equations $\\ref{5.84}$ and $\\ref{5.85}$ denote the the total partial derivative of the loss function, $L$, with respect to $y$ and $h$, respectively. $\\chi$ and $\\varphi$ constitute important auxiliary gradient sequences in the 'backward pass', which I deal with in detail subsequently in section 5.6 LSTM system derivatives. In practice, $L$ may be defined as the sum of the respective contributions of $y$, thus:\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    L = \\sum^{K_m - 1}_{x = 0} L \\left( y_x \\right)\n",
    "    \\label{5.86} \\tag{5.86}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Furthermore, $h_x$, at $x = k$, has been shown to influence $h_x$, at $x = k + 1$ (see for example Pascanu, Mikolov & Bengio (2013)). The dependency of $h_{x+1}$ on $h_x$ through $y_x$ may be shown by rewriting equation $\\ref{5.80}$ thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_{x+1} &= W_y y_x + W_x x_m \\left( x \\right) + b_h \\label{5.87} \\tag{5.87} \\\\ \\\\ \n",
    "    y_x &= G \\left( h_x \\right) \\\\ \\\\\n",
    "    y_{x+1} &= G \\left( h_{t+1} \\right) \\\\ \\\\\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Thus, use of the chain rule in this way permits the description of the total partial derivative of $L$ with respect to $h_x$ and $y_x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\chi \\left( x \\right) &= \\frac {\\partial L y_x} {\\partial y_x} + W_y \\varphi_x  \\label{5.88} \\tag{5.88} \\\\ \\\\\n",
    "    \\varphi \\left( x \\right) &= \\chi_x \\ast \\frac {dJ \\left( z \\right)} {dz} \\label{5.89} \\tag{5.89} \\\\ \\\\\n",
    "    &= \\left( \\frac {\\partial L \\left( y_x \\right)} {\\partial y_x} + W_y \\varphi_{x+1} \\right) \\ast \\frac {dG \\left( z \\right)} {dz} \\label {5.90} \\tag{5.90}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The sequence formed by the total partial derivative of $L$ with respect to $h_x$ and $y_x$, which is generally designated as the 'error gradient' in literature, requires $\\ref{5.88}$ be initialized too, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\varphi \\left( x = K_m \\right) = 0 \\label{5.91} \\tag{5.91}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I may now write the derivatives of the model's parameters by applying the chain rule to $\\ref{5.80}$ and using $\\ref{5.90}$, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac {\\partial L} {\\partial \\Theta} \\left( x \\right) &= \\left\\{ \\frac {\\partial L} {\\partial W_y} \\left( x \\right), \\frac {\\partial L} {\\partial W_x} \\left( x \\right), \\frac {\\partial E} {\\partial b_h} \\left( x \\right) \\right\\} \\label {5.92} \\tag{5.92}  \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_y} \\left( x \\right) &= \\varphi_x y^T \\left( x - 1 \\right) \\label {5.93} \\tag{5.93}  \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_x} \\left( x \\right) &= \\varphi_x x^T_m \\left( x \\right) \\label {5.94} \\tag{5.94}  \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial b_h} \\left( x \\right) &= \\varphi_x  \\label {5.95} \\tag{5.95}  \\\\ \\\\\n",
    "    \\frac {dL} {d \\Theta} &= \\sum^{K_m - 1}_{x = 0} \\frac {\\partial L} {\\partial \\Theta} \\left( x \\right) \\label {5.96} \\tag{5.96}  \\\\ \\\\\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The unrolled RNN cell shares the same set of model parameters, $\\Theta$, for all steps. Consequently, the total derivative of $L$, with respect to $\\Theta$, includes all steps, as illustrated by equation $\\ref{5.96}$, which may now be used as part of optimization by GD. Furthermore, the quantities required to update the parameters of the system, $\\Theta$, are, per equations $\\ref{5.93}$, $\\ref{5.94}$, and $\\ref{5.95}$, directly proportional to $\\varphi_x$. When training a RNN using BP, GD flows in the reverse direction of the index, $x$ and the fraction of $\\varphi_x$ retained from BP is particularly important. This component of $L$ is responsible for adjusting the model's parameters, $\\Theta$. I may expand the recursion in $\\ref{5.90}$ to yield:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac {\\partial \\varphi_x} {\\partial \\varphi_l} = \\prod^l_{k = x + 1} W_y \\ast \\frac {dJ \\left( z \\right)} {dz}  \\label {5.97} \\tag{5.97}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sherstinsky (2020) explains that the Jacobian matrix in $\\ref{5.97}$, $\\frac {\\partial \\varphi_x} {\\partial \\varphi_l}$, results from $l - x$ individual Jacobians, $W_y \\ast \\frac {dJ \\left( z \\right)} {dz}$. If the eigenvalues, $\\mu_i$, of $W_y$, meet the stability requirement, $0 < \\mu_i < 1$, then $\\left\\| W_y \\right\\| < 1$. Furthermore, $\\left\\| \\frac {dJ \\left( z \\right)} {dz} \\right\\| < 1$, which follows the choice of normalization function (see equations $\\ref{5.17}$, $\\ref{5.18}$, $\\ref{5.39}$, and $\\ref{5.40}$). Thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\left\\| \\frac {\\partial \\varphi_x} {\\partial \\varphi_l} \\right\\| &\\sim \\left( \\left\\| W_y \\right\\| \\ast \\left\\| \\frac {dJ \\left( z \\right)} {dz} \\right\\| \\right)^{1 - x} \\label {5.98} \\tag{5.98} \\\\ \\\\ \n",
    "    &\\sim \\left\\| W_y \\right\\|^{1 - x} \\ast \\left\\| \\frac {dJ \\left( z \\right)} {dz} \\right\\|^{1 - x} \\approx 0  \\label {5.99} \\tag{5.99}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "But should at least one eigenvalue of $W_y$ breach the stability requirement, $\\left\\| W_y \\right\\|^{1 - x}$ will grow exponentially, resulting in two possible outcomes: the vanishing, or exploding, gradients, discussed in section 3.1 deep learning history and topography. In the first case, $h_x$ grows, $y_x$ saturates, resulting in $\\left\\| \\frac {\\partial \\varphi_x} {\\partial \\varphi_l} \\right\\| \\approx 0$. The second case is rarer and results in $h_x$ biasing so that $\\frac {dJ \\left( z \\right)} {dz} \\neq 0$. If $x_m \\left(x \\right)$ keeps the RNN on this course for a large number of steps, $\\left\\| \\frac {\\partial \\varphi_x} {\\partial \\varphi_l} \\right\\|$ grows, potentially overflowing. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I note in section 3.1 that much research, particularly in the 1990’s and 2000’s, was motivated by the above insight (see for example Pascanu, Mikolov & Bengio (2013), Hochreiter, Bengio, Paolo & Schmidhuber (2001), Hochreiter & Schmidhuber (1997)). The most effective solution so far (see for example Goodfellow, Bengio & Courville (2016)) is the long short-term memory (LSTM) cell architecture originally proposed by Hochreiter & Schmidhuber (1997).\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## The Long short-term memory recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Section 5.2 explains how the RNN, unrolled for $K_m$ steps, processes the parts, or segments, of it, one at a time. Equation $\\ref{5.79}$ initializes $h$, the cell output. Equation $\\ref{5.82}$ then selects the input samples, following which equations $\\ref{5.80}$ and $\\ref{5.81}$ are applied for $K_m$ steps, $0 \\leq x \\leq K_m - 1$. The same process is applied recursively until all the parts have been processed. For the rest of this paper, for brevity, I do not reproduce the segment notation of equation $\\ref{5.80}$, so that henceforth all operations may be regarded to be on a fully unrolled cell, that is to say, whereby the index of the individual observations, $x$, travels all its steps, $0 \\leq x \\leq K_m - 1$, for all $x$, in $0 \\leq m \\leq M - 1$.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I have shown in the earlier sections 5. Model, how, in the RNN system, $y_x$ is a normalized version of $h_x$. The weighted $y_x$ is fed back recursively as part of the update of $h_x$. The intimate relationship between $y_x$ and $h_x$ through this recursion directly impacts the gradient of $L$. The impact accumulates via training and results in the 'vanishing' (or more rarely 'exploding') gradient phenomena. The key insight of the long short-term memory recurrent neural network (LSTM RNN or LSTM) design is the incorporation of 'controls' directly into the network cell. The controls, which take the form of 'gates', may be trained in a manner that acts as a brake upon the gradient of the loss function with respect to $h$, i.e., the value that is proportional to parameter updates obtained from training by GD, and thus obviate the vanishing or exploding phenomena (see Schmidhuber (2015, 2013), Grossberg (2013), Perez-Ortiz, Gers, Eck, & Schmidhuber (2003), Hochreiter, Bengio, Paolo & Schmidhuber (2001), Gers, Schmidhuber & Cummins (2000), Hochreiter & Schmidhuber (1997)). The LSTM modifies the RNN cell in several important ways.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Recall equation $\\ref{5.37}$, which I now re-write thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_x &= \\mathcal{G}_h \\left( h_{x - 1} \\right) + \\mathcal{G}_u \\left( y_{x - 1}, x_x \\right) \\label{5.100} \\tag{5.100} \\\\ \\\\\n",
    "    y_x &= J \\left( h_x \\right) \\label{5.101} \\tag{5.101} \\\\ \\\\\n",
    "    \\mathcal{G}_h \\left( h_{x - 1} \\right) &= W_h h_{x-1}  \\label{5.102} \\tag{5.102} \\\\ \\\\\n",
    "    \\mathcal{G}_u \\left( y_{x-1}, x_x \\right) &= W_y y_{x-1} + W_x x_x + b_h  \\label{5.103} \\tag{5.103}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $J \\left( z \\right)$ is the normalization function $\\mathrm{tanh}$ (see equations $\\ref{5.17}$ and $\\ref{5.18}$). $\\mathcal{G}_h \\left( h_x - 1 \\right)$ carries forward the value of $h$ from the previous step. $\\mathcal{G}_u \\left( r \\left[ n - 1 \\right], x \\left[ n \\right] \\right)$, represents update (subscript '$u$') information, both normalized output, $y$ and input $x$ at the respective index $x$, plus the bias, $b_h$. In $\\ref{5.100}$, $h$ combines the information in equal proportion at each step, but the proportions are rendered adjustable by the introduction of two new 'gate' signals, $f^g_x$, a so-called 'forget gate', and $i^g_x$, a so-called 'input gate', respectively, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_x &= f^g_x \\ast \\mathcal{G}_h \\left( h_{x-1} \\right) + i^g_x \\ast \\mathcal{G}_u \\left( y_{x-1}, x_x \\right) \\label{5.104} \\tag{5.104} \\\\ \\\\\n",
    "    0 &\\leq f^g_x, i^g_x  \\leq 1 \\label{5.105} \\tag{5.105}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Figure 3 illustrates the expansion of the canonical RNN system through the addition of a 'forget gate' $f^g_x$, to control the amount of $h$ retained from the previous step, and the addition of an 'input gate', $i^g_x$, to control the amount of input at the current step.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Figure 3. Introducing gates, i.e. the 'forget gate' and 'input gate', to the canonical RNN system](fig_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The forget and input gates in equations $\\ref{5.104}$ and $\\ref{5.105}$ provide the means by which the contributions of input $x$ and the carried foward state of the cell $h$ may be precisely controlled at every step. $W_h$ in $\\ref{5.102}$ comprises positive fractions $\\frac {1} {|a_{ii}|}$ on its main diagonal and because the elements of $f^g_x$ are also fractions, I may write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    W_h = I \\label{5.106} \\tag{5.106}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "in $f^g_x \\ast W_h$, providing that the gate functions have parameters that are learned during training. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I may simplify $\\ref{5.102}$ thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\mathcal{G}_s \\left( h_{x-1} \\right) = h_{x-1} \\label{5.107} \\tag{5.107}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "so that equation $\\ref{5.104}$ may be written:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    h_x &= f^g_x  \\ast \\mathcal{G}_h \\left( h_{x-1} \\right) + i^g_x \\ast \\mathcal{G}_u \\left( y_{x-1}, x_x \\right)  \\\\ \\\\\n",
    "    &= f^g_x \\ast y_{x-1} + i^g_x \\ast \\mathcal{G}_u \\left( y_{x-1}, x_x \\right) \\label{5.108} \\tag{5.108} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "With the result that the update term, $\\mathcal{G}_u \\left( y_{x-1}, x_x \\right)$, is now controlled by the forget gate $f^g_x$. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Turning to the internal composition of $\\mathcal{F}_u \\left( y_{x-1}, x_x \\right)$, equation $\\ref{5.103}$ states that $y$ from the previous step and $x$ at the current step contribute equally. However, always using $W_y y_{x-1}$ is potentially problematic because when $i^g_x \\sim 1$, $h_{x-1}$  $y_x$ become connected through $W_y$ and the associated normalization. Following $\\ref{5.97}$, this makes the system susceptible to the vanishing gradient phenomenon.  \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This potential feedback path was eliminated by Hochreiter (1991) and Hochreiter & Schmidhuber (1997) by the introduction of a third gate, the output gate, which I denote $o^g_x$, thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    l_x &= o^g_x \\ast y_x   \\label{5.109} \\tag{5.109} \\\\ \\\\\n",
    "    0 &\\leq o^g_x \\leq 1  \\label{5.110} \\tag{5.110} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The output gate determines the cell's observable value at step $x$.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Substituting $l_{x-1}$ for $y_x$ in $\\ref{5.103}$ transforms it thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\mathcal{G}_u \\left( l_{x-1}, x_x \\right) = W_y l_{x-1} + W_x x_x + b_h \\label{5.111} \\tag{5.111}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Figure 4 illustrates the expansion of the canonical RNN system through the further addition of the 'output gate' $o^g_x$, per equation $\\ref{5.109}$, and its recursive relationship, equation $\\ref{5.111}$.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Figure 4. The 'output gate' determines the amount of signal that becomes the cell's observable value at the current step](fig_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Equation $\\ref{5.111}$ may be transposed by multiplying $x$ in $\\ref{5.103}$ by the dedicated input gate $i^g_x$, thus:\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\mathcal{G}_u \\left( l_x, x_x \\right) = W_y l_{x-1} + i^g_x x_x \\ast W_x x_x + b_h \\label{5.112} \\tag{5.112}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Equations $\\ref{5.109}$ and $\\ref{5.112}$ show that the output gate, $o^g_x$, and the input gate, $i^g_x$, allow the update term, $\\mathcal{G}_u \\left( l_{x-1}, x_x \\right)$, to contain aspects pf both output and input values.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The value of $l_x \\left[ n \\right]$ is determined by the output $y_x$, in turn bounded by normalization, $J_d \\left( z \\right)$. To keep the same range and incorporate the input $i^g_x \\ast x_x$, it is necessary that $\\mathcal{G}_u \\left( l_{x-1}, x_x \\right)$, is also normalized using $J \\left( z \\right)$, to provide the 'update' $u_x$, thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    u_x = J \\left( \\mathcal{G}_u \\left( l_{x-1}, x_x \\right) \\right) \\label{5.113} \\tag{5.113}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The 'update' term in $\\ref{5.108}$ may be transposed with $u_x$ in $\\ref{5.113}$, to yield:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "     h_x = f^g_x \\ast h_{x-1} i^g_x \\ast u_x \\label{5.114} \\tag{5.114} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Equation $\\ref{5.114}$ is part of a set of formulas that fully describe the LSTM network.  It shows that $h_x$ is a weighted combination of the value of the cell at the previous step and the cumulative information newly available at the present step.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The core framework of the LSTM cell is thus described by the enhancement of the canonical RNN system via the addition of the forget, input, and output gates, illustrated in figure 5.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Figure 5. LSTM network showing $h_x$ as a weighted combination of $h_{x-1}$ and the aggregation of historical and novel update information available at the present step](fig_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I now define expressions for $f^g_x$, $i^g_x$, and $o^g_x$, as shown in figure 5.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I assume that the LSTM will be trained with BP so that the gates are differentiable.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Consider now the logistic function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    J \\left( z \\right) &\\equiv \\sigma \\left( z \\right) \\equiv \\frac {1} {1 + e^{-z}} \\label{5.115} \\tag{5.115} \\\\ \\\\\n",
    "    &= \\frac {1 + \\mathrm{tanh} \\left( \\frac {z} {2} \\right) } {2} \\label{5.116} \\tag{5.116} \\\\ \\\\\n",
    "    &= \\frac {1 + J \\left( \\frac {z} {2} \\right) } {2} \\label{5.117} \\tag{5.117}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "as a transformation of the hyperbolic tangent, the normalization function, $J \\left( z \\right)$. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To determine fractional values for the forget, input, and output gates, $f^g_x$, $i^g_x$, and $o^g_x$ respectively, all preceding information may be utilized by the LSTM. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For both $f^g_x$, the forget gate, which determines the contribution to the cell's value, $h_{x-1}$, from the previous step, and the input gate, $i^g_x$, which determines the contribution to cell's value from the current step, the available information is $h_{x-1}$, $l_{x-1}$, and $x_x$.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "However, $o^g_x$ differs from the other two gates by having available to it $h_x$, $y_{x-1}$, and $x_x$. This results from $\\ref{5.109}$, which ensures that $y_x$ is available and $\\ref{5.101}$, which ensures $h_x$ is available. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The input to each gate may therefore be written thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    z_{f^g_x} &= W_{x_{f^g_x}} x_x + W_{h_{i^g_x}} h_{x-1} + b_{f^g_x} \\label{5.118} \\tag{5.118} \\\\ \\\\\n",
    "    z_{i^g_x} &= W_{x_{i^g_x}} x_x + W_{h_{i^g_x}} h_{x-1} + b_{i^g_x} \\label{5.119} \\tag{5.119} \\\\ \\\\\n",
    "    z_{o^g_x} &= W_{x_{o^g_x}} x_x + W_{h_{o^g_x}} h_x + b_{o^g_x} + \\theta_{cr} \\label{5.120} \\tag{5.120} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "So that it follows that, as the weights are determined through training via BP,  the gate functions may be written thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    f^g_x &= J \\left( z_{f^g_x} \\right) \\label{5.121} \\tag{5.121} \\\\ \\\\\n",
    "    i^g_x &= J \\left( z_{i^g_x} \\right) \\label{5.122} \\tag{5.122} \\\\ \\\\\n",
    "    o^g_x &= J \\left( z_{o^g_x} \\right) \\label{5.123} \\tag{5.123} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The gates constitute the seminal contribution of Hochreiter and Schmidhuber (1997) to DL.  The gates modulate training data through the LSTM system, thus enabling  the LSTM cell to interpret and obviate local optima and artificial boundaries that arise in the unrolling of the RNN and lead to the vanishing gradient phenomenon (Sherstinsky (2018, 2020), Schmidhuber (2013), Hochreiter & Schmidhuber (1997)). The gates robustify the LSTM system and enable it to handle phenomena in data and generate high quality output sequences. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## LSTM mechanism in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sherstinsky (2020) posits that the LSTM cell fundamentally handles two potentially competing yet symbiotic objectives: (i) data; and (ii) data control (Sherstinsky, 2020). The data, $y_x$, $h_x$, and $x_x$, is in the normalized range $-1$ to $1$) and data control signals from the gates are also normalized in the range $0$ to $1$. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Element-wise multiplication of the data vectors by the control vectors provides the mechanism by which data is propagated optimally to nodes within the LSTM cell, so that, when, for example, the control signal is 0.6, 60% of data is passed on to the nodes. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Figure 6 illustrates the main operations of the LSTM cell that have been developed so far:\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Figure 6. LSTM network cell](fig_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "It may also be useful at this stage to summarize the notation that I have used to describe the LSTM cell so far. It may be summarized thus:\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$x$ denotes the index of an individual observation in a segment or window, $n = 0, ..., K - 1$\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$K$ denotes the number of observations\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$x_x$ denotes the input to the cell \n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$h_x$ denotes the output of the cell \n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$y_x$ denotes the normalized output of the cell \n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$J \\left(\\cdot \\right)$ denotes a normalization function \n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$L$ denotes the loss function\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$a$ denotes an accumulation node\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$u$ denotes an update to the state of the cell \n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$g$ denotes a gate \n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$f^g_x$ denotes the forget gate\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$i^g_x$ denotes the input gate\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$o^g_x$ denotes the output gate \n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{f^g_x}$ denotes the weight of the forget gate associated with the input (or output, and normalized output) respectively\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{i^g_x}$ denotes the weight of the input gate associated with the input (or, again, the output, and normalized output) respectively\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{o^g_x}$ denotes the weight of the output gate associated with the input (or, again as above, the output, and normalized output) respectively\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$b_{f^g_x}$ denotes the bias parameter, $b$, associated with the forget gate\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$b_{i^g_x}$ denotes the bias parameter, $b$, associated with the input gate\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$b_{o^g_x}$ denotes the bias parameter, $b$, associated with the output gate\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "It is necessary to standardize data within the LSTM cell in order that all observations within the the training dataset input to it are mean $0$ and standard deviation $1$. This may be achieved in practice thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\mu &= \\frac {1} {K} \\sum^{K - 1}_{k = 0} x_x \\label{5.124} \\tag{5.124} \\\\ \\\\\n",
    "    \\mathcal{K} &= \\frac {1} {K - 1} \\sum^{K - 1}_{k = 0} \\left( x_x - \\mu \\right) {\\left( x_x - \\mu \\right)}^T \\label{5.125} \\tag{5.125} \\\\ \\\\\n",
    "    x_x &= {\\left( \\mathrm{diag} \\left[ \\sqrt{\\mathcal{V}} \\right] \\right)}^{- 1} \\left( x_x - \\mu \\right) \\label{5.126} \\tag{5.126} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $K$ denotes the number of observations, $mu$ denotes the sample mean, $\\mathcal{K}$ denotes an auto-covariance matrix for the training dataset.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In earlier sections of 5. Model I set out the rationale for use of the normalization function $J \\left( \\cdot \\right)$ to output values between $0$ and $1$. The Sigmoidal, or logistic, function is widely used by researchers and practitioners to achieve this (see for example Chollet (2018), Goodfellow, Bengio & Courville (2016), and Grossberg (2013)) and may be computed thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    J \\left( z \\right) &\\equiv \\sigma \\left( z \\right) \\\\ \\\\\n",
    "    \\sigma \\left( z \\right) &\\equiv \\frac {1} {1 + e^{- z}}  = \\frac {1 + \\mathrm{tanh} \\left( \\frac {z} {2} \\right)} {2} = \\frac {1 + J \\left( \\frac {z} {2} \\right)} {2} \\label{5.127} \\tag{5.127}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The hyperbolic tangent is also widely used by researchers and practitioners and may be computed thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    J \\left( z \\right) &\\equiv \\mathrm{tanh} \\left( z \\right) \\\\ \\\\ \n",
    "    \\mathrm{tanh} \\left( z \\right) &= \\frac {e^z - e^{- z}} {e^z + e^{- z}} = 2 \\sigma \\left( 2 z \\right) - 1 = 2 J \\left( 2 z \\right) - 1 \\label{5.122} \\tag{5.128}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Finally, I may combine and/or reassemble the architecture of the LSTM cell described above into more formalized set of 15 parameter entities, thus: \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{x^i_x}$ denotes weights for the input, $x_x$, at the input gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{x^f_x}$ denotes weights for the state of the cell or cell output, $h_x$, at the input gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{x^o_x}$ denotes weights for the normalized out output, $y_x$, at the input gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$b_{i^g_x}$ denotes the bias parameter, $b$, associated with the input gate\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{h^i_x}$ denotes weights for the input, $x_x$, at the forget gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{h^f_x}$ denotes weights for the state of the cell or cell output, $h_x$, at the forget gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{h^o_x}$ denotes weights for the normalized out output, $y_x$, at the forget gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$b_{f^g_x}$ denotes the bias parameter, $b$, associated with the forget gate\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{y^i_x}$ denotes weights for the input, $x_x$, at the output gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{y^f_x}$ denotes weights for the state of the cell or cell output, $h_x$, at the output gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{y^o_x}$ denotes weights for the normalized out output, $y_x$, at the output gate.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$b_{o^g_x}$ denotes the bias parameter, $b$, associated with the output gate\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{u^a_x}$ denotes weights for the 'data update' accumulation node.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$W_{u^l_x}$ denotes the weights connecting the cell state or output to the 'data update' accumulation node.\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$b_{u^a_x}$ denotes the bias parameter for the 'data update' accumulation node.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The parameters of the LSTM network are generally denoted collectively by $\\Theta$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\Theta \\equiv \\left\\{ W_{x^i_x}, W_{x^f_x}, W_{x^o_x}, b_{i^g_x}, W_{h^i_x}, W_{h^f_x}, W_{h^o_x}, b_{f^g_x}, W_{y^i_x}, W_{y^f_x}, W_{y^o_x}, b_{o^g_x}, W_{u^a_x}, W_{u^l_x}, b_{u^a_x} \\right\\}\n",
    "    \\label{5.129} \\tag{5.129}\n",
    "\\end{align*}\n",
    "$\\\\[0.03in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$\\Theta$ may also be usefully written thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    A = \\begin{Bmatrix}\n",
    "    W_{x^i_x} & W_{x^f_x} & W_{x^o_x} & b_{i^g_x} \\\\\n",
    "    W_{h^i_x} & W_{h^f_x} & W_{h^o_x} & b_{f^g_x} \\\\\n",
    "    W_{y^i_x} & W_{y^f_x} & W_{y^o_x} & b_{o^g_x} \\\\\n",
    "    W_{u^a_x} & & W_{u^l_x} & b_{u^a_x}\n",
    "    \\end{Bmatrix}\n",
    "    \\label{5.130} \\tag{5.130}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## The forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The forward pass is a fundamental aspect of 'machine learning' and denotes the group of computations whereby the LSTM cell may be unrolled for $K$ steps in order to produce sequences of values or samples. The forward pass therefore shows how the values that characterize the step of the cell at index $x$, are based upon the values at $x - 1$. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The LSTM may be fully described by the following equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    a_{i^g_x} &=  W_{x^i_x} + W_{x^f_x} + W_{x^o_x} + b_{i^g_x} \\label{5.131} \\tag{5.131} \\\\ \\\\\n",
    "    a_{f^g_x} &=  W_{h^i_x} + W_{h^f_x} + W_{h^o_x} + b_{f^g_x} \\label{5.132} \\tag{5.132} \\\\ \\\\\n",
    "    a_{o^g_x} &=  W_{y^i_x} + W_{y^f_x} + W_{y^o_x} + b_{o^g_x} \\label{5.133} \\tag{5.133} \\\\ \\\\\n",
    "    a_{i^g_x} &=  W_{u^a_x} + W_{u^l_x} + b_{u^a_x} \\label{5.134} \\tag{5.134} \\\\ \\\\\n",
    "    u^a_x &= J \\left( a_{i^g_x} \\right) \\label{5.135} \\tag{5.135} \\\\ \\\\\n",
    "    g_{i^g_x} &= J \\left( a_{i^g_x} \\right) \\label{5.136} \\tag{5.136} \\\\ \\\\\n",
    "    g_{f^g_x} &= J \\left( a_{f^g_x} \\right) \\label{5.137} \\tag{5.137} \\\\ \\\\\n",
    "    g_{o^g_x} &= J \\left( a_{o^g_x} \\right) \\label{5.138} \\tag{5.138} \\\\ \\\\\n",
    "    h_x &= g_{f^g_x} \\ast h_{x-1} + g_{i^g_x}] \\ast u^a_x \\label{5.139} \\tag{5.139} \\\\ \\\\\n",
    "    y_x &= J \\left( h_x \\right) \\label{5.140} \\tag{5.140} \\\\ \\\\\n",
    "    l_x &= g_{o^g_x} \\ast y_x \\label{5.141} \\tag{5.141} \\\\ \\\\\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Figure 7 illustrates the LSTM cell, $\\ref{5.131}$ - $\\ref{5.141}$, unrolled over 4 steps.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Figure 7. The sequence of steps generated by unrolling a cell of the LSTM network](fig_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Figure 8 shows in full the operation of the LSTM cell and highlights specific functions as verticals. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Figure 8. LSTM cell showing the stages of the system](fig_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Backward propagation through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Backward propagation, or backward propagation through time (both BP) is another fundamental process of ML and constitutes the mechanisms via which models are trained. Generally following the framework of Sherstinsky (2020), I describe the equations that are necessary for training the LSTM cell using BP. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To obtain update equations two supplementary gradient sequences that move in backward direction (hence 'backward propagation') may be computed.  The first: $\\chi_x$, denotes the total partial derivative of the loss function, $L$, with respect $l_x$, whilst the second $\\varphi_x$, denotes the total partial derivative of $L$ with respect to $h_x$ (recall, $\\chi$ and $\\varphi$ were introduced in section 5.3 Vanishing gradients (see for example equations $\\ref{5.84}$ and $\\ref{5.85}$)).\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$\\chi_x$ and $\\varphi_x$ bestow important architectural (implementation) benefits. Expressing each intra-cell total partial derivative in terms of $\\chi_x$, instead of explicitly computing the total partial derivative of the objective function, $L$, with respect to each variable of the cell (Palangi, Deng, Shen, Gao, He, Chen, Song & Ward (2015) provide an example of this alternate approach), reduces the number of intermediate variables. As a result, the equations for the backward pass are more straightforward and architecturally more streamlined and easier to implement (see Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia, Jozefowicz, Kaiser, Kudlur, Levenberg, Mane, Monga, Moore, Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan, Viegas, Vinyals, Warden, Wattenberg, Wicke, Yu, & Zheng (2015), re TensorFlow's modularization of the backward pass, which codifies this approach, for example).\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The gradient sequences propagate backwards, in the direction opposite to that of the state signal, $h_x$. As a result, $\\chi_x$ and $\\varphi_x$ follow a backward-moving recursion and their respective values depend upon the values of the same quantities at the index $x + 1$. Once known, $\\chi_x$ and $\\varphi_x$ may be used to compute the total partial derivatives of $L$ with respect to accumulation nodes or intermediate gradient sequences, which I denote $\\alpha_{f^g_x}$, $\\alpha_{i^g_x}$, $\\alpha_{o^g_x}$, and $\\alpha_{u^a_x}$. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The LSTM cell takes the input signal at each step and computes the normalized output signal for all steps.  Other values are used internally within the cell, with the exception of $h_x$ and $l_x$, which naturally traverse steps within a sequence or window. $l_x$ may be transformed to provide the normalized output $y_x$. Similarly, the input, $h_x$, may be derived via transformation of the training dataset.  For example, in the empirical study, I use a weighted-mid-price (WMP) that is, in turn, computed from thoroughly cleansed and prepared high frequency stock and foreign exchanged price innovations. I also pay considerable attention in section 6.1 Implementation to 'windowing', the process whereby my high frequency input dataset is organized into segments of input samples that lie within selected 'windows', surrounding the given step and combined so as to enhance the overall system's 'attention' to context.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I write the derivatives of the normalizing functions following $\\ref{5.127}$ and $\\ref{5.128}$, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac {dJ_c \\left( z \\right)} {dz} &= J_c \\left( z \\right) \\left( 1 - J_c \\left( z \\right) \\right) \\label{5.142} \\tag{5.142} \\\\ \\\\\n",
    "    \\frac {dJ_d \\left( z \\right)} {dz} &= 1 - {\\left( J_d \\left( z \\right) \\right)}^2 \\label{5.143} \\tag{5.143}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where the subscripts $d$ and $u$ denote data and control processes respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I then define $\\chi_x$ as the total partial derivative of the loss function, $L$, with respect to $l_x$ thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\chi_x \\equiv \\nabla_{l_x} L = \\frac {\\partial L} {\\partial l_x} \\label{5.144} \\tag{5.144} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I may also define $\\partial L$ with respect to three intermediate variables, $\\rho_x$, $\\gamma_x$, and $\\alpha_x$, respectively, plus $\\varphi_x$, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\rho_x &\\equiv \\nabla_{y_x} L = \\frac {\\partial L} {\\partial y_x} \\label{5.145} \\tag{5.145} \\\\ \\\\\n",
    "    \\gamma_x &\\equiv \\nabla_{g_x} L = \\frac {\\partial L} {\\partial g_x} \\label{5.146} \\tag{5.146} \\\\ \\\\\n",
    "    \\alpha_x &\\equiv \\nabla_{a_x} L = \\frac {\\partial L} {\\partial a_x} \\label{5.147} \\tag{5.147} \\\\ \\\\\n",
    "    \\varphi_x &\\equiv \\nabla_{h_x} L = \\frac {\\partial L} {\\partial h_x} \\label{5.148} \\tag{5.148} \\\\ \\\\\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$\\varphi_x$ is particularly important because, as the total partial derivative of the loss function with respect to the cell's state $h_x$ all parameter updates in the LSTM cell depend upon it.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Equations $\\ref{5.131}$ - $\\ref{5.141}$ provide the BP equations by using $\\ref{5.144}$ - $\\ref{5.148}$ to apply the chain rule, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\chi_x &= {\\left( \\frac {\\partial y_x} {\\partial l_x} \\right)}^T \\left( \\frac {\\partial L} {\\partial y_x} \\right) + f_{\\chi_x} \\label{5.149} \\tag{5.149} \\\\ \\\\\n",
    "    \\rho_x &= {\\left( \\frac {\\partial l_x} {\\partial y_x} \\right)}^T \\left( \\frac {\\partial L} {\\partial l_x} \\right) = \\left( \\nabla_{l_x} L \\right) \\ast o^g_x = \\chi_x \\ast o^g_x  \\label{5.150} \\tag{5.150} \\\\ \\\\\n",
    "    \\gamma_x &= \\frac {\\partial L} {\\partial l_x} \\frac {\\partial l_x} {\\partial o^g_x} = \\left( \\nabla_{l_x} L \\right) \\ast y_x = \\chi_x \\ast y_x  \\label{5.151} \\tag{5.151} \\\\ \\\\\n",
    "    \\alpha_x &= \\gamma_x \\ast \\frac {\\partial o^g_x} {\\partial \\alpha_{o^g_x}} = \\gamma_x \\ast \\frac {dJ \\left( z \\right)} {dz} = \\chi_x \\ast y_x \\ast \\frac {dJ \\left( z \\right)} {dz} \\label{5.152} \\tag{5.152} \\\\ \\\\\n",
    "    \\varphi_x &= \\rho_x \\ast \\frac {\\partial y_x} {\\partial h_x} + \\frac {\\partial \\alpha_{o^g_x}} {\\partial h_x} \\alpha_{o^g_x} + r_{\\varphi_{x+1}} \\label{5.153} \\tag{5.153} \\\\ \\\\\n",
    "    &= \\rho_x \\ast {J \\left( z \\right)} {dz} + W_{h_{o^g_x}} \\alpha_{o^g_x} + r_{\\varphi_{x+1}} \\label{5.154} \\tag{5.154} \\\\ \\\\\n",
    "    &= \\chi_x \\ast o^g_x \\ast \\frac {dJ \\left( z \\right)} {dz} + W_{s_{o^g_x}} \\alpha_{o^g_x} + r_{\\varphi_{x+1}} \\label{5.155} \\tag{5.155} \\\\ \\\\\n",
    "    \\alpha_{f^g_x} &= \\varphi_x \\ast \\frac {\\partial h_x} {\\partial f^g_x} \\ast \\frac {\\partial f^g_x} {\\partial \\alpha_{f^g_x}} = \\varphi_x \\ast h_{t-1} \\ast \\frac {dJ \\left( z \\right)} {dz} \\label{5.156} \\tag{5.156} \\\\ \\\\\n",
    "    \\alpha_{i^g_x} &= \\varphi_x \\ast \\frac {\\partial h_x} {\\partial i^g_x} \\ast \\frac {\\partial i^g_x} {\\partial \\alpha_{i^g_x}} = \\varphi_x \\ast u_x \\ast \\frac {dJ \\left( z \\right)} {dz} \\label{5.157} \\tag{5.157} \\\\ \\\\\n",
    "    \\alpha_{u^a_x} &= \\varphi_x \\ast \\frac {\\partial h_x} {\\partial u_x} \\ast \\frac {dJ \\left( z \\right)} {dz} = \\varphi_x \\ast i^g_x \\ast \\frac {dJ \\left( z \\right)} {dz} \\label{5.158} \\tag{5.158}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    r_{\\chi_{x+1}} &= W_{l_{i^g_x}} \\alpha_{{i^g_x}_{x+1}} + W_{l_{f^g_x}} \\alpha_{{f^g_x}_{x+1}} + W_{l_{o^g_x}} \\alpha_{{o^g_x}_{x+1}} + W_{l_{u^a_x}} \\alpha_{{u^a_x}_{x+1}} \\label{5.159} \\tag{5.159} \\\\ \\\\\n",
    "    r_{\\varphi_{x+1}} &= W_{h_{i^g_x}} \\alpha_{{i^g_x}_{x+1}} + W_{h_{i^g_x}} \\alpha_{{f^g_x}_{x+1}} + {f^g_x}_{x+1} \\ast \\varphi_{x+1} \\label{5.160} \\tag{5.160} \\\\ \\\\\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "are fractions of the total derivative of $L$ with respect to $y_x$ and $h_x$ at $x+1$.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The total partial derivatives of $L$ with respect to $\\Theta$, i.e. $\\ref{5.129}$, are proportional to $\\ref{5.152}$, $\\ref{5.156}$, $\\ref{5.157}$, and $\\ref{5.158}$. Thus, from $\\ref{5.131}$ - $\\ref{5.141}$, I have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac {\\partial L} {\\partial W_{x_{i^g_x}}} &= \\alpha_{i^g_x} x^T_x \\label{5.161} \\tag{5.161} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{s_{i^g_x}}} &= \\alpha_{i^g_x} h^T_{x-1} \\label{5.162} \\tag{5.162} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{l_{i^g_x}}} &= \\alpha_{i^g_x} l^T_{x-1} \\label{5.163} \\tag{5.163} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial b_{i^g_x}} &= \\alpha_{i^g_x} \\label{5.164} \\tag{5.164} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{x_{f^g_x}}} &= \\alpha_{f^g_x} x^T_x \\label{5.165} \\tag{5.165} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{h_{f^g_x}}} &= \\alpha_{f^g_x} h^T_{x-1} \\label{5.166} \\tag{5.166} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{l_{f^g_x}}} &= \\alpha_{f^g_x} l^T_{x-1} \\label{5.167} \\tag{5.167} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial b_{f^g_x}} &= \\alpha_{f^g_x} \\label{5.168} \\tag{5.168} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{x_{o^g_x}}} &= \\alpha_{o^g_x} x^T_x \\label{5.169} \\tag{5.169} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{h_{o^g_x}}} &= \\alpha_{o^g_x} h^T_x \\label{5.170} \\tag{5.170} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{l_{o^g_x}}} &= \\alpha_{o^g_x} l^T_{x-1} \\label{5.171} \\tag{5.171} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial b_{o^g_x}} &= \\alpha_{o^g_x} \\label{5.172} \\tag{5.172} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{x_{u^a_x}}} &= \\alpha_{u^a_x} h^T_x \\label{5.173} \\tag{5.173} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{l_{u^a_x}}} &= \\alpha_{u^a_x} l^T_{x-1} \\label{5.174} \\tag{5.174} \\\\ \\\\\n",
    "    \\frac {\\partial L} {\\partial b_{u^a_x}} &= \\alpha_{u^a_x} \\label{5.175} \\tag{5.175} \\\\ \\\\\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Re-arranged to the form of $\\ref{5.130}$, $L$ with respect to $\\Theta$, at index $x$, may be written thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac {\\partial L} {\\partial \\Theta} \\left( x \\right) = \\begin{Bmatrix}\n",
    "    \\frac {\\partial L} {\\partial W_{x_{i^g_x}}} & \\frac {\\partial L} {\\partial W_{s_{i^g_x}}} & \\frac {\\partial L} {\\partial W_{l_{i^g_x}}} & \\frac {\\partial L} {\\partial b_{i^g_x}} \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{x_{f^g_x}}} & \\frac {\\partial L} {\\partial W_{h_{f^g_x}}} & \\frac {\\partial L} {\\partial W_{l_{f^g_x}}} & \\frac {\\partial L} {\\partial b_{f^g_x}} \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{x_{o^g_x}}} & \\frac {\\partial L} {\\partial W_{h_{o^g_x}}} & \\frac {\\partial L} {\\partial W_{l_{o^g_x}}} & \\frac {\\partial L} {\\partial b_{o^g_x}} \\\\\n",
    "    \\frac {\\partial L} {\\partial W_{x_{u^a_x}}} & & \\frac {\\partial L} {\\partial W_{l_{u^a_x}}} & \\frac {\\partial L} {\\partial b_{u^a_x}}\n",
    "    \\end{Bmatrix}\n",
    "    \\label{5.176} \\tag{5.176}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "When unrolling a LSTM cell for $K$ observations, i.e. a full window of the training dataset, $\\Theta$, is shared by all steps because $\\Theta$ applies to the cell as a whole. As a result $L$ aggregates all steps in the window, thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac {d L} {d \\Theta} = \\sum^{K = 1}_{n = 0} \\frac {\\partial L} {\\partial \\Theta} \\left( x \\right)\n",
    "    \\label{5.177} \\tag{5.177}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$\\ref{5.177}$ may now be used by GD and in the empirical setting $\\ref{5.177}$ is computed for the window and then supplied to the GD optimization algorithm.  \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In the preceding sections I have been able to show, by generally following the framework set out by Sherstinsky (2020), why the introduction of the gates Hochreiter & Schmidhuber (1997) makes the LSTM network better suited to GD training than the standard RNN. GD works best when the elements of $\\frac {\\partial L} {\\partial \\Theta} \\left( x \\right)$ are well-behaved numerically. By implication, $\\alpha_{f^g_x}$,  $\\alpha_{i^g_x}$, $\\alpha_{o^g_x}$, and $\\alpha_{u^a_x}$, along with, $\\chi_x$ and $\\varphi_x$, are required to be robust over the range of $K$ steps. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Expanding $\\ref{5.155}$ yields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\varphi_x & = \\chi_x \\ast o^g_t \\ast \\frac {dJ \\left( z \\right)} {dz} + W_{h_{o^g_x}} \\chi_x  \\ast y_x \\ast \\frac {dJ \\left( z \\right)} {dz} \n",
    "    + r_{\\varphi_{x+1}} \\label{5.178} \\tag{5.178} \\\\ \\\\\n",
    "    &= \\left[{\\left( \\frac {\\partial y_x} {\\partial l_x} \\right)}^T \\left( \\frac {\\partial L} {\\partial y_x} \\right) + r_{\\chi_{x+1}} \\right] \n",
    "    \\ast o^g_x \\frac {dJ \\left( z \\right)} {dz} \\\\ \n",
    "    &+ W_{h_{o^g_x}} \\left[{\\left( \\frac {\\partial y_x} {\\partial l_x} \\right)}^T \\left( \\frac {\\partial L} {\\partial y_x} \\right) + r_{\\chi_{t+1}} \\right] \\ast y_x \\ast \\frac {dJ \\left( z \\right)} {dz} + r_{\\varphi_{x+1}} \n",
    "    \\label{5.179} \\tag{5.179}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "It follows from $\\ref{5.159}$ and $\\ref{5.160}$ that $\\chi_x$ and $\\varphi_x$ depend upon $\\varphi_{x+1}$. I therefore repeat the approach taken in section 5.3 'Vanishing gradients', shift indexes $x \\longrightarrow k - 1$, and apply the chain rule to $\\ref{5.179}, to yield:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac {\\partial \\varphi_{k - 1}} {\\partial \\varphi_k} \n",
    "    & = \\left( \\frac {\\partial \\varphi_{k-1}} {\\partial r_{\\chi_k}} \\right) \\left( \\frac {\\partial r_\\chi} {\\partial \\varphi_k} \\right)\n",
    "    + \\left( \\frac {\\partial \\varphi_{k-1}} {\\partial r_{\\varphi_k}} \\right) \\left( \\frac {\\partial r_{\\varphi_k}} {\\partial \\varphi_k} \\right)\n",
    "    \\label{5.180} \\tag{5.180} \\\\ \\\\\n",
    "    & = \\left( \\frac {\\partial \\varphi_{k-1}} {\\partial r_{\\chi_k}]} \\right) \n",
    "    \\left\\{\n",
    "    \\left( \\frac {\\partial r_{\\chi_k}} {\\partial \\alpha_{i^g_k}} \\right) \\left( \\frac {\\partial \\alpha_{i^g_k}} {\\partial \\varphi_k} \\right) +\n",
    "    \\left( \\frac {\\partial r_{\\chi_k}} {\\partial \\alpha_{f^g_k}} \\right) \\left( \\frac {\\partial \\alpha_{f^g_k}} {\\partial \\varphi_k]} \\right) +\n",
    "    \\left( \\frac {\\partial r_{\\chi_k}} {\\partial \\alpha_{u^a_k}} \\right) \\left( \\frac {\\partial \\alpha_{u^a_k}} {\\partial \\varphi_k]} \\right)    \n",
    "    \\right\\} \\\\\n",
    "    & + \\; \\left( \\frac {\\partial \\varphi_{k-1}} {\\partial r_{\\varphi_k}} \\right) \n",
    "    \\left\\{\n",
    "    \\left( \\frac {\\partial r_{\\varphi_k}} {\\partial \\alpha_{i^g_k}} \\right) \\left( \\frac {\\partial \\alpha_{i^g_k}} {\\partial \\varphi_k} \\right) +\n",
    "    \\left( \\frac {\\partial r_{varphi_k}} {\\partial \\alpha_{f^g_k}} \\right) \\left( \\frac {\\partial \\alpha_{f^g_k}} {\\partial \\varphi_k} \\right) +\n",
    "    \\mathrm{diag} \\bigg[ f^g_k \\bigg]\n",
    "    \\right\\}    \n",
    "    \\label{5.181} \\tag{5.181} \\\\ \\\\\n",
    "    &= \\left( \n",
    "    \\mathrm{diag} \\bigg[ {o^g_k}_{k-1} \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]  \n",
    "    + W_{h_{o^g_k}} \\mathrm{diag} \\bigg[ y_{k-1} \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg] \n",
    "    \\right) \\\\\n",
    "    &\\times \\left\\{ \n",
    "    W_{l_{i^g_k}} \\mathrm{diag} \\bigg[ u_k \\bigg] \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]  \n",
    "    + W_{l_{f^g_k}} \\mathrm{diag} \\bigg[ h_{k-1} \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg] \n",
    "    + W_{l_{u^a_k}} \\mathrm{diag} \\bigg[ i^g_k \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg] \n",
    "    \\right\\} \\\\\n",
    "    &+ \\left(\n",
    "    W_{h_{i^g_k}} \\mathrm{diag} \\bigg[ u_k \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]\n",
    "    + W_{h_{f^g_k}} \\mathrm{diag} \\bigg[ h_{k-1} \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]\n",
    "    + \\mathrm{diag} \\bigg[ f^g_k \\bigg]\n",
    "    \\right)\n",
    "    \\label{5.182} \\tag{5.182} \\\\ \\\\\n",
    "    &=  \\mathrm{diag} \\bigg[ u_n \\ast \\frac {dJ} {dz} \\bigg]\n",
    "    \\times \\left\\{\n",
    "    W_{l_{f^g_k}} \\left( \n",
    "    \\mathrm{diag} \\bigg[ o^g_k \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]\n",
    "    +  W_{h_{o^g_k}} \\mathrm{diag} \\bigg[y_{k-1} \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]\n",
    "    \\right) + W_{h_{i^g_k}} \n",
    "    \\right\\} \\\\\n",
    "    &+  \\mathrm{diag} \\bigg[ h_{k-1} \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]\n",
    "    \\times \\left\\{\n",
    "    W_{l_{f^g_k}} \\left( \n",
    "    \\mathrm{diag} \\bigg[ o^g_k \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]\n",
    "    +  W_{h_{o^g_k}} \\mathrm{diag} \\bigg[y_{k-1} \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]\n",
    "    \\right) + W_{h_{f^g_k}} \n",
    "    \\right\\} \\\\\n",
    "    &+  \\mathrm{diag} \\bigg[ i^g_k \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]\n",
    "    \\times \\left\\{\n",
    "    W_{l_{a^u_k}} \\left( \n",
    "    \\mathrm{diag} \\bigg[ o^g_k \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]\n",
    "    + W_{h_{o^g_k}} \\mathrm{diag} \\bigg[ y_{k-1} \\ast \\frac {dJ \\left( z \\right)} {dz} \\bigg]\n",
    "    \\right) \\right\\} \\mathrm{diag} \\bigg[ f^g_k \\bigg]\n",
    "    \\label{5.183} \\tag{5.183} \\\\ \\\\\n",
    "    &= \\mathcal{J} \\left( k - 1, k ; \\Theta \\right) + \\mathrm{diag} \\bigg[ f^g_k \\bigg]\n",
    "    \\label{5.184} \\tag{5.184} \n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $\\Theta = \\left\\{ W_{h_{i^g_x}}, W_{l_{i^g_x}}, W_{h_{f^g_k}}, W_{l_{f^g_k}}, W_{h_{o^g_k}}, W_{l_{u^a_k}} \\right\\}$ and $\\mathcal{J} \\left( k - 1, k ; \\Theta \\right)$ comprise all the terms in $\\frac {\\partial \\varphi_{k-1}} {\\partial \\varphi_k}$, with the exception of $\\mathrm{diag} \\bigg[ f^g_k \\bigg]$, and $\\mathrm{diag} \\bigg[ \\mathcal{v} \\bigg]$ denotes a diagonal matrix in which vector $\\mathcal{v}$ occupies the main diagonal.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To train the LSTM, I process $\\frac {\\partial \\varphi_{k-1}} {\\partial \\varphi_k}$ from index $x$ to $z \\leq K - 1$, where $z \\gg n$, thus:\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\frac {\\partial \\varphi_x} {\\partial \\varphi_z} = \\prod^z_{k = x + 1} \\frac {\\partial \\varphi_{k-1}} {\\partial \\varphi_k} \n",
    "    \\label{5.185} \\tag{5.185}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Recall the seminal contribution of the LSTM network, namely obviating 'vanishing gradients' (I continue to assume for convenience that the far less common problem of exploding gradients may be dealt with separately). I may take the value of $L$ with respect to $h_z$, so that $\\ref{5.185}$ in fact determines how much of the gradient sequence did not 'vanish' during training and instead was held by $\\varphi_x$ and thus available to update the LSTM's parameters.  \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Empirical study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## LSTM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The empirical study was conducted on a single machine, using an i7-9750H CPU @ 2.60GHz (12 CPUs) and 16384 MB RAM, plus NVIDIA GeForce RTX 260 GPU, with 1920 CUDA cores, a memory data rate of 14.00 Gbps, and total graphics memory of 14259 MB. I implemented certain functions associated with the dataset preparation with GPU acceleration, using CuPy libraries, and I implemented the algorithms using the opensource packages  Numpy (Harris, C.R., Millman, K.J., van der Walt, S.J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N.J., Kern, R., Picus, M., Hoyer, S., Brett, M., Haldane, A., Fernandez del Rio, A., Wiebe, M., Sheppard, K., Reddy, T., Weckesser, W. & Oliphant, T.E. (2020)), Keras (Chollet, F. (2015)), and tensorflow (Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y. & Zheng, X. (2015)).\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The longest compute time requirement for this research was 162:37:06 [hh:mm:ss], to estimate EURUSD with the comparison model, $\\mathrm{MSM} \\left( 13 \\right)$, using 13 volatility frequencies (i.e. $\\bar k = 13$, thus $\\mathrm{MSM} \\left( 13 \\right)$). The reason for the length of time taken may be explained as follows. Denote $M_t$ as a volatility state vector, where each component $M_{k, t}$ is a state variable. In section 6.2 'Comparison model $\\mathrm{MSM} \\left( \\bar k \\right)$' below I show that the econometrician observes the returns $x_t = \\sigma {\\left[ g \\left( M_t \\right) \\right]}^\\frac {1} {2} \\epsilon_t$, but not the vector $M_t$ itself; the vector $M_t$ is latent and thus inferred recursively by Bayesian updating. From an implementation perspective, the Bayesian recursion is one of two major obstacles to successful implementation of MSM with HF data (the other being the construction of the transition matrix $A$). Since it is not possible to vectorize the recursion, other means of speeding up the recursion must be found (NB: Vectorized code performs operations upon multiple components of a vector at the same time (in one statement) instead of looping. Vectorizing loops can be extremely difficult, but can reduce very long processing times to virtually instantaneous). In my review of the multifractal literature in the companion paper (Collins (2020)) I report that 13 of 31 multifractal papers used daily or weekly prices and 6 used simulated data only. 8 studies were theoretical and used no data. Only 5 studies used HF data (in these respects, i.e. the majority of studies use relatively small, tractable datasets, the multifractal literature is similar to the finance DL literature). Typically datasets used for estimating and forecasting MSM (and similarly LSTM) consisted of a few thousand observations. In this context, a recursion (loop) does not impose an unacceptable computational overhead. But with HF datasets approaching hundreds of thousands of observations or more, it very quickly becomes intractable, especially so when each iteration of the loop involves the fresh construction of a high-dimensional state space (i.e. a high value, say $\\geq 11$, has been ascribed to $\\bar k$). By contrast, comparable compute times for the LSTM were somewhat quicker. The LSTM network formulation is highly tunable in implementation, which enables more to be done in order to reduce compute times. The longest compute time requirement for the LSTM was 22:21:32, being the 2-day forecast for the EURUSD 1-minute HF dataset.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The LSTM cell shown in figure 8 and fully defined by equations $\\ref{5.126}$ - $\\ref{5.136}$ is implemented as a network of many cells (thus NN). The overall 'size', or capacity or parameterization, of a NN is determined by its depth, that is the number of hidden layers, which I may denote $M$, and its width, the number of cells in each layer $\\left\\{N_1, N_2, ..., N_M \\right\\}$. An important theoretical result for LSTMs is that their formulation is general enough to represent any desired function (LeCun, Bengio, & Hinton (2015)). Given at least one hidden layer and a sufficiently large but finite number of cells, this is feasible by the arguments of the universal approximation theorem (see Hornik, Stinchcombe & White (1989)), whereby a function that is Borel measurable and defined over finite-dimensional spaces (thus continuous on a compact sub-set of $\\mathbb{R}^n$) can be approximated with arbitrary precision. By implication LSTMs may account for the non-linearity of the empirical phenomena being modeled. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I split the data into train, validation, and out-of-sample testing, using a (70%, 20%, 10%) split for the training, validation, and test sets, as is common in DL studies.  For this research I have of course sequential data, i.e. each observation is dependent upon the other observations immediately prior to it. Thus, one training example includes several (specifically, an arbitrarily selected, following considerable experimentation, 60, or one hour) of consecutive observations, ${\\left\\{ y_{t-1} \\right\\}}^{59}_{i=o}$. Predictions over the validation and testing sub-datasets were made multiple steps ahead. Non-linearity of LSTM warrants direct (as opposed to iterated) forecasts for each point in the prediction horizon (1-hour, 1-day, 2 day respectively, to align with the companion paper). I use gradient descent (see equations $\\ref{5.173}$ - $\\ref{5.182}$), with an arbitrarily, again following considerable experimentation, 32 training examples per batch. Then, 1 training cycle (or 'epoch') is composed of a full run through the training dataset, thus feeding the training examples for all possible $t$. The source code for my implementation of the LSTM contains additional functionality regarding implementation, including routines for 'data windowing', the process of mapping my data inputs to outputs, and in the process of doing so, establishing a 3-dimensional data structure that is required by the tensorflow package. In the interests of brevity, I defer readers who may interested in this detail to my github (https://github.com/johncollinsedhec/markov-switching-multifractal), which also provides the source code for the comparison model, $\\mathrm{MSM} \\left( \\bar k \\right)$, and the HF data cleansing and preparation.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "From theory, it is known that an insufficiently sized LSTM underfits data. Standard statistical fit measures demonstrate poor performance, even in-sample. Conversely, too large a LSTM may overfit the data and statistical fit measures will demonstrate strong performance in-sample, but collapse out-of-sample. In the latter case, the econometrician may regard the LSTM as having memorized the data without learning its high level structure and thus acquiring the ability to generalize the accumulated knowledge. Architecture size and structure in the empirical setting is therefore based upon exploration, which amounts to experimenting with different model specifications. This exploration may be guided by the relative predictive performance of the specifications considered. Usually, predictive fit is determined upon a data sample separate and distinct from training (in-) and testing (out-of-) samples. In practice, it is generally easier to start with a larger LSTM, allowing for redundancy and erring toward an excess of parameters, to then restrict excessive depth/width via regularization. Somewhat counter-intuitively, deep architectures also tend to be easier to implement and cheaper to train (i.e. faster with less computational overhead). \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "An important role is played by training and regularization techniques (see for example, Srivastava, Hinton, Krizhevsky, Sutskever & Salakhutdinov (2014)). Regularization improves generalization of a LSTM and addresses potential overfit in the model. Literature describes many regularization techniques (for example, see Goodfellow, Bengio & Courville (2016) and Chollet (2018)). Those generally regarded as most effective are based upon the working presumption that good models are invariant to small perturbations. Dropout is a widely use regularization method that adds noise to the LSTM architecture (Srivastava, et al. (2014)). During a dropout training process, units and connections are randomly dropped, usually from hidden layers, but under some circumstances from visible layers too. Dropout is loosely analogous to model averaging (Srivastava, et al. (2014) develop the analogy). Data augmentation is an alternative regularization method that encourages fitting to relevant features of the data, as distinct from irrelevant noise. In a data augmentation process, data is expanded with synthetic data comprising the original data plus contaminants, usually in the from of Guassian/white noise. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Loss minimization algorithms (Goodfellow, Bengio & Courville (2016) provide an extensive review) are usually variations of a standard gradient descent with some added stochasticity. The granularity of data inputs used for gradient computations and parameter updates may vary from the use of a full dataset (\"batch gradient descent\") to several (\"mini-batch gradient descent\") or single (\"stochastic gradient descent\") randomly chosen block(s) of observations. Using smaller batches prevents use of the most efficient computational tools and can lead to noisier convergence, but the built in stochasticity helps to avoid local minima and thus also works as a regularizer (Chollet (2018)). Minimization algorithms utilize efficient ways for computing gradients that improve upon the canonical chain rule (BP - see equations $\\ref{5.137}$ - $\\ref{5.172}$). Several methods may be used to ensure satisfactory convergence of minimization algorithms, that is, a convergence that is acceptably fast and avoids local minima. They include \"learning rate decay\", whereby parameters are updated in the face of stochasticity inherent in the minimization algorithm used, so that the learning rate is high far from the minimum in order to ensure fast learning, falling as the minimum is approached to prevent overshooting (i.e. \"learning rate decay\"). Parameter updating may be additionally employed to maintain memory of recent updates so that improvement continues in the most promising direction (\"momentum\" - see Bengio, et al. (2007)). Finally, though somewhat crude, the econometrician learns that an outright termination of the algorithm's iterations well before the minimum is reached can save a considerable amount of time and computational overhead with little downside (formalized as \"early stopping\", Prechelt (1997)). All of these methods again contribute to regularization.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Comparison model $\\mathrm{MSM} \\left(\\bar{k} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I use Mincer-Zarnowitz regressions, $R^2$, and MSE to compare out-of-sample forecasts from the LSTM to out-of-sample forecasts from the Markov-switching multifractal stochastic volatility model of Calvet & Fisher (2004, 2008). I summarize my implementation of the $\\mathrm{MSM} \\left(\\bar{k} \\right)$ briefly below and refer the reader to Collins (2020) for full details.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Following equations $\\ref{4.1}$ and $\\ref{4.2}$, I may define the innovations $x_t \\equiv \\mathrm{WMP}_t - \\mathrm{WMP}_{t-1}$ and consider them within an economy that has $\\bar k$ components $M_{1,t}, M_{2,t}, \\: ..., \\: M_{\\bar k,t}$, which decay at heterogeneous frequencies $\\gamma_1, \\gamma_2, ..., \\gamma_{\\bar k}$. I model the innovations $x_t \\equiv \\mathrm{WMP}_t - \\mathrm{WMP}_{t-1}$ as $x_t = \\sigma \\left( M_{1,t}, M_{2,t}, ..., M_{\\bar k,t} \\right)^\\frac {1}{2} \\epsilon_t$. The parameter $\\sigma$ is a positive constant. The random variables $\\epsilon_t$ are $\\mathrm{i.i.d.}$ standard Guassian $\\mathcal{N}(0,1)$. The random multipliers or volatility components $M_{k,t}$ are persistent, non-negative and satisfy $\\mathbb{E} \\left( M_{k,t} \\right) = 1$. The multipliers $M_{1,t}, M_{2,t}, \\: ..., \\: M_{\\bar k,t}$ at a given time $t$ may be considered to be statistically independent, and the parameter $\\sigma$ is thus equal to the unconditional standard deviation of the innovation $x_t$. I stack the period $t$ volatility multipliers into a vector $M_t = \\left( M_{1,t}, M_{2,t}, ..., M_{\\bar k,t} \\right)$.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For any $m = \\left( m_1, m_2, ..., m_{\\bar k} \\right) \\in \\mathbb{R}^{\\bar k}$, let $g \\left( m \\right)$ denote the product $\\prod_{i=1}^{\\bar k} m_i$. Volatility at time $t$ is then $\\sigma_t = \\sigma {\\left[ g \\left( M_t \\right) \\right]}^\\frac {1} {2}$. The properties of volatility are driven by the stochastic dynamics of the vector $M_t$. To simplify the set up Calvet & Fisher assume that $M_t$ is first-order Markov, which facilitates the construction through time of $\\{ x_t \\}$ and permits maximum likelihood estimation. $M_t$ is denoted as a volatility state vector, whilst each component $M_{k, t}$ is denoted as a state variable. The econometrician observes the returns $x_t = \\sigma {\\left[ g \\left( M_t \\right) \\right]}^\\frac {1} {2} \\epsilon_t$, but not the vector $M_t$ itself. The vector $M_t$ is therefore latent, and must be inferred recursively by Bayesian updating. Each $M_{k,t}$ follows a process that is identical except for time scale.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Assume that volatility state variables have been constructed up to date $t - 1$. For each $k \\in \\{ 1, 2, ..., \\bar k \\}$, the next period multiplier $M_{k,t}$ is drawn from a fixed distribution $M$ with probability $\\gamma_k$ and is otherwise equal to its current value: $M_{k,t} = M_{k, t-1}$. The dynamics of $M_{k,t}$ may therefore be summarized thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    M_{k,t} =\n",
    "    \\begin{cases}\n",
    "    \\text{ drawn from distribution M }  & \\text{ with probability } \\gamma_k \\\\\n",
    "    M_{k,t-1} & \\text { with probability }  1 - \\gamma_k\n",
    "    \\end{cases}\n",
    "    \\label{6.1} \\tag{6.1}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The transition probabilities $\\gamma \\equiv \\left( \\gamma_1, \\gamma_2, ..., \\gamma_{\\bar k} \\right)$ may be specified as $\\gamma_k = 1 - \\left( 1 - \\gamma_1 \\right)^{b^{k - 1}}$, where $\\gamma_1 \\in \\left( 0, 1 \\right)$ and $b \\in \\left( 1, \\infty \\right)$. Since $1 - \\gamma_k = {\\left( 1 - \\gamma_1 \\right)}^{b^{k-1}}$, the logarithms of staying probabilities are exponentially decreasing with $k$. The parameter $\\gamma_{\\bar k}$ thus denotes the high frequency switching probability, whilst $b$ denotes the frequency growth rate. In this setting, a process with very persistent components (equivalently a very small parameter $\\gamma_1$), with small values of $k$, also has small $\\gamma_1 b^{k-1}$, so that the transition probability satisfies $\\gamma_k \\sim \\gamma_1 b^{k-1}$. At higher frequencies $\\left( \\gamma_k \\sim 1 \\right)$, the rate of increase slows down and $\\gamma_k = 1 - \\left( 1 - \\gamma_1 \\right)^{b^{k - 1}}$ guarantees that the parameter $\\gamma_k$ remains lower than 1. In empirical settings, it is numerically convenient to estimate parameters with the same order of magnitude. Since $\\gamma_1 < \\; ... \\; < \\gamma_{\\bar k} < 1 < b$, the set of transition probabilities is specified by $\\left( \\gamma_{\\bar k}, b \\right)$. The parameter $\\bar k$ determines the number of volatility frequencies in the model. It is fixed, and thus a model selection problem.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The multifractal construction imposes only minimal restrictions on the marginal distribution of the multipliers: $M \\geq 0$ and $\\mathbb{E} \\left( M \\right) = 1$. I use the simple binomial set up for this research, where binomial random variables take the values $m_0$ or $2 - m_0$ with equal probability. The full parameter vector is thus $\\psi \\equiv \\left( m_0, \\sigma, b, \\gamma_{\\bar k} \\right) \\in \\mathbb{R}^4$, where $m_0$ characterizes the distribution of the multipliers, $\\sigma$ is the unconditional standard deviation of $\\mathrm{WMP}$ returns, and $b$ and $\\gamma_{\\bar k}$ define the set of switching probabilities. I have shown that, in my set up, the multiplier $M$ has a discrete distribution, therefore finite number of volatility states. Standard filtering methods thus provide the likelihood function in closed form. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let the multiplier $M$ take a finite number of values $b_m$, so that the vector $M_t = \\left( M_{1, t}, M_{2, t}, ..., M_{{\\bar k}, t} \\right)$ may therefore take $d = b^{\\bar k}_m$ values, $m^1, m^2, ..., m^d \\in \\mathbb{R}^{\\bar k}$. The dynamics of the Markov chain $M_t$ are characterized by a transition matrix $A = {\\left( a_{i,j} \\right)}_{1 \\leq {i, j} \\leq d}$, with components $a_{i, j} = \\mathbb{P} \\left(M_{t+1} = m^j \\mid M_t = m^i \\right)$. I construct these components thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    a_{i, j} = \\prod^{\\bar k}_{k = 1} \\left[ \\left( 1 - \\gamma_k \\right) 1_\\{m^i_k = m^j_k\\} + \\gamma_k \\mathbb{P} \\left( M = m^j_k \\right) \\right]\n",
    "    \\label{6.2} \\tag{6.2}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $m^i_k$ denotes the $m$th component of vector $m^i$, and $1_\\{m^i_k = m^j_k\\}$ is a dummy variable equal to 1 if $m^i_k = m^j_k$ and 0 otherwise. The density of the innovation $x_t$ conditional on $M_t$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    f_{x_t} \\left( x \\mid M_t = m^i \\right) = {\\left[ \\sigma g \\left( m^i \\right) \\right]}^-1 n \\left[ \\frac {x} {\\sigma g \\left( m^i \\right)} \\right]\n",
    "    \\label{6.3} \\tag{6.3}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $n \\left(.\\right)$ denotes the density of a standard normal. The conditional probablities $\\prod^j_t \\equiv \\mathbb{P} \\left( M_t = m^j \\mid x_1, ..., x_t \\right)$ over the unobserved states $m^1, m^2, ..., m^d$ can be stacked in the row vector $\\prod_t = \\left( \\prod^1_t, \\prod^2_t, ..., \\prod^d_t \\right) \\in \\mathbb{R}^d_+$. Letting $\\iota = \\left( 1, ..., 1 \\right) \\in \\mathbb{R}^d$, I know that $\\prod_t i\\prime = 1$. I use Bayes' rule to update $\\prod_{t+1}$ (the new belief) from $\\prod_t$ (the old belief) and the new innovation $x_{t+1}$, using the function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\omega \\left( x_t \\right) = \\left[ \\frac {n \\left( \\frac {x_t} {\\sigma g \\left( m^1 \\right)} \\right)} {\\sigma g \\left( m^1 \\right)}, \\frac {n \\left( \\frac {x_t} {\\sigma g \\left( m^2 \\right)} \\right)} {\\sigma g \\left( m^2 \\right)}, ..., \\frac {n \\left( \\frac {x_t} {\\sigma g \\left( m^d \\right)} \\right)} {\\sigma g \\left( m^d \\right)} \\right]\n",
    "    \\label{6.4} \\tag{6.4}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "From equation $\\ref{6.4}$ I can express the conditional probability $\\prod_{t+1}$ as a function of the observation $x_{t+1}$ and the probability $\\prod_t$ calculated in period $t$, thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\Pi_{t+1} = \\frac {\\omega \\left( x_{t+1} \\right) \\ast \\left( \\prod_t A \\right)} {\\left[ \\omega \\left( x_{t+1} \\right) \\ast \\left( \\prod_t A \\right) \\right] \\iota\\prime}\n",
    "    \\label{6.5} \\tag{6.5}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $x \\ast y$ denotes the Hadamard product $\\left( x_1 y_1, x_2 y_2, ..., x_d y_d \\right)$ of any $x, y \\in \\mathbb{R}^d$. With this formulation $\\prod_t$ may be recursively calculated. Since the multipliers $\\left( M_{1, 1}, M_{2, 1}, ..., M_{{\\bar k}, 1} \\right)$ are independent, the initial vector $\\prod_0$ is uniquely determined by $\\prod^j_0 = \\prod^{\\bar k}_{l = 1} \\mathbb{P} \\left(M = m^j_l \\right)$ for all $j$.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Relative performance assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Out-of-sample performance of the LSTM versus $\\mathrm{MSM} \\left(\\bar{k} \\right)$ is assessed via volatility forecasts. I conduct typical full-sample Mincer & Zarnowitz (1969) forecast rationality tests, following the approach commonly used in the literature (see Diebold (2017) and Patton & Sheppard (2009) for extensive reviews of the method). Writing the target value to forecast at time $t$ using information up to time $t-h$ as $y_t$ and denoting the forecast by $y_{t \\mid t-h}$, the Mincer & Zarnowitz (MZ) regression may be written thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    x^2_{t+h} = \\gamma_0 + \\gamma_1 \\mathbb{E} x^2_{t+h} + u_{t, h}\n",
    "    \\label{6.6} \\tag{6.6}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $h$ is the forecast horizon and $u_{t,h}$ is the residual. If the forecasts are unbiased, the constant $\\gamma_0$ should be statistically insignificantly different from zero; if the forecasts are optimal, the slope $\\gamma_1$ should be statistically insignificantly different from unity. The $\\mathrm{MSE}$ quantifies forecast errors, thus: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    \\mathrm{MSE} = L^{-1} \\sum^T_{t=T-L+1} {\\left( x^2_t - \\mathbb{E}_{t-1} x^2_t \\right)}^2\n",
    "    \\label{6.7} \\tag{6.7}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "And the coefficient of determination is defined thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    R^2 = 1 - \\frac {\\mathrm{MSE}} {L^{-1} \\sum^T_{t=T-L+1} {\\left( x^2_t - \\sum^T_{i=T-L+1}  x^2_i / L \\right)}^2} \n",
    "    \\label{6.8} \\tag{6.8}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "My null hypothesis therefore is that $\\gamma_0 = 0$ and $\\gamma_1 = 1$, jointly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\\begin{align*}\n",
    "    H_0 \\: \\: : \\: \\: \\gamma_0 = 0 \\cap \\gamma_1 = 1\n",
    "    \\label{6.9} \\tag{6.9}\n",
    "\\end{align*}\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For a good forecast, therefore, $\\gamma_0 \\approx 0$, $\\gamma_1 \\approx 1$, $\\mathrm{MSE}$ is low, and $R^2$ is high.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\newpage\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Table 1 reports results for LSTM and $\\mathrm{MSM} \\left( 13 \\right)$.  I use **bold font** to show when $\\gamma_0$ is closer to zero, $\\gamma_1$ is closer to 1, $\\mathrm{MSE}$ is lower, or $R^2$ is higher, respectively, for each model.  \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "*Table 1. Comparison of out-of-sample forecasting performance at different horizons*\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "| | | $\\gamma_0$ | $\\gamma_1$ | $\\mathrm{MSE}$ | $\\mathrm{R^2}$ |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| **LSTM**\n",
    "| 1-hour forecast | AAPL 1-min | 0.02327 | 0.53216 | 0.08836 | 0.10061 |\n",
    "| |                            | *(0.01466)* | *(0.72885)* |||\n",
    "|  | JPM 1-min                 | 0.04953 | 0.85523 | 1.53371 | 0.04723 | \n",
    "| |                            | *(0.01955)* | *(0.04887)* |||\n",
    "|  | EURUSD 1-min              | **0.01363** | 0.76612 | 0.90003 | 0.02896 |\n",
    "| |                            | *(0.41778)* | *(0.12375)* |||\n",
    "| 1-day forecast | AAPL 1-min  | -0.18641 | 0.63217 | 0.45623 | 0.07184 |\n",
    "| |                            | *(0.09656)* | *(0.06635)* |||\n",
    "|  | JPM 1-min                 | 0.46321 | 0.81032 | 11.63694 | 0.06651 | \n",
    "| |                            | *(0.93025)* | *(0.03825)* |||\n",
    "|  | EURUSD 1-min              | **0.11632** | **1.03279** | 6.11883 | 0.09662 |  \n",
    "| |                            | *(0.06230)* | *(0.07036)* |||\n",
    "| 2-day forcast | AAPL 1-min   | 0.30395 | 0.63589 | 3.79192 | 0.03071 | \n",
    "| |                            | *(0.07836)* | *(0.03723)* |||\n",
    "|  | JPM 1-min                 | 0.39940 | 0.86521 | 62.65874 | 0.09254 |\n",
    "| |                            | *(0.07365)* | *(0.11895)* |||\n",
    "|  | EURUSD 1-min              | 0.35698 | 0.67188 | 41.32156 | 0.42589 |\n",
    "| |                            | *(0.16328)* | *(0.12774)* |||\n",
    "| **$\\mathrm{MSM} \\left( 13 \\right)$**\n",
    "| 1-hour forecast | AAPL 1-min | **0.00898** | **1.08343** | **0.06417** | **0.10998** |\n",
    "| |                            | *(0.01654)* | *(0.05406)* |||\n",
    "|  | JPM 1-min                 | **-0.01379**  | **1.05521** | **1.28774** | **0.06895** | \n",
    "| |                            | *(0.01247)* | *(0.03625)* |||\n",
    "|  | EURUSD 1-min              | 0.01494 | **0.90628** | **0.81499** | **0.03421** |  \n",
    "| |                            | *(0.013228)* | *(0.07333)* |||\n",
    "| 1-day forecast | AAPL 1-min  | **-0.01864** | **1.21308** | **0.36624** | **0.35009** |\n",
    "| |                            | *(0.09526)* | *(0.17036)* |||\n",
    "|  | JPM 1-min                 | **0.05944** | **0.99225** | **9.34175** | **0.14636** | \n",
    "| |                            | *(0.05602)* | *(0.01177)* |||\n",
    "|  | EURUSD 1-min              | -0.12277 | 0.96545 | **4.86687** | **0.11296** |\n",
    "| |                            | *(0.03650)* | *(0.12146)* |||\n",
    "| 2-day forcast | AAPL 1-min   | **-0.27941** | **1.37277** | **2.93887** | **0.47623** | \n",
    "| |                            | *(0.04963)* | *(0.09927)* |||\n",
    "|  | JPM 1-min                 | **-0.16135** | **1.01485** | **56.36200** | **0.26185** |\n",
    "| |                            | *(0.04162)* | *(0.14498)* |||\n",
    "|  | EURUSD 1-min              | **-0.20544** | **1.12382** | **30.10264** | **0.20078** |  \n",
    "| |                            | *(0.06325)* | *(0.23456)* |||\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Table 1 shows that $\\mathrm{MSM} \\left( 13 \\right)$ provided comparatively better forecasts than LSTM in almost all tests for AAPL, JPM, and EURUSD over 1-hour, 1-day, and 2-day horizons.  Standard errors are shown in (*italics and parenthesis*). The hypothesis $\\gamma_0 = 0$ and $\\gamma_1 = 1$ was accepted at a 5% confidence level in the $\\mathrm{MSM} \\left( 13 \\right)$ case for all series at all horizons. The estimated intercept $\\hat \\gamma_0$ is slightly above or below zero, but these small biases were not statistically significant. The MZ regressions thus show little evidence of bias in $\\mathrm{MSM} \\left( 13 \\right)$ forecasts with high frequency data that has accounted for microstructure noise. The point estimates for LSTM are somewhat more biased than for $\\mathrm{MSM} \\left( 13 \\right)$ in most cases. Some biases were also statistically significant, so that the null hypothesis was rejected at 5% for 1-hour forecasts for AAPL and EURUSD, and 1-day forecasts for AAPL and JPM. $\\mathrm{MSM} \\left( 13 \\right)$ produced better $\\mathrm{MSE}$ and $R^2$ scores in all cases.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The observed LSTM performance is broadly in line with literature. Due to its inherent non-linearity, LSTM discovered, in an unsupervised manner, the different volatility regimes and provided reasonable out-of-sample performance. However, $\\mathrm{MSM} \\left( 13 \\right)$, provides comparatively better forecasts than LSTM almost all the time. I attribute these findings primarily to $\\mathrm{MSM} \\left( \\bar k \\right)$'s stronger underlying theoretical structure (i.e. the multifractal volatility approach better accounts for the stylized facts: almost unpredictable returns, fat tails, and volatility clustering) and $\\mathrm{MSM} \\left( 13 \\right)$'s superior fittingness to the data. \n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The LSTM literature outside of finance has generally shown that LSTM efficiently captures long-range memory and non-linear dependencies within various types of sequential data (see for example Lipton, Berkowitz, & Elkan (2015)).  LSTM literature thus provides a general consensus regarding the unreasonable effectiveness of NNs in a broader context (Karpathy (2015)). Possible explanations for this are multifarious, ranging from statistical physics (Mehta & Schwab (2014)), to geometry (Lei, Zhang & Artzi (2018)), information theory (Schwartz-Ziv & Tishby (2017)), and Bayesian statistics (see for example Patel, Nguyen & Baraniuk (2015)). This contrasts with finance, where research has generally shown that DL models, and NNs and LSTMs in particular, do not clearly outperform historically predominant deterministic and stochastic approaches. Verstyuk (2019) notes that without particular care to implementation (the 'AI trinity' data, compute, and algorithm (Anandkumar (2019)), DL models are generally shown to be less accurate than alternative statistical approaches in the financial time series literature. A recent extensive study by Gu, Kelly & Xiu (2019a) by contrast concludes that machine learning shows great promise for empirical asset pricing. These authors report similarly superior performance for NNs in an asset pricing context (Gu, Kelly & Xiu (2019b)).\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Section 3.2 'Deep learning in finance' includes six recent finance studies that focus upon LSTM in broadly comparable settings to my research and I recap the findings of these studies now. Generally LSTM outperforms comparison models; however, the comparison models may arguably be weaker than $\\mathrm{MSM} \\left( 13 \\right)$. Firstly, Almosova & Andresen (2019) examined average forecasting performance using inflation data and compared LSTM to several alternative models using rolling-window real time forecasts. This research showed that LSTM outperformed all other forecasting techniques, especially at longer horizons with the RMSFE being approximately one third to one halve of the errors for the benchmark models. Secondly, Borovkova & Tsiamas (2019) proposed an ensemble of LSTM for intraday stock price direction predictions, using a large variety of technical analysis indicators as network inputs. The LSTM ensemble operated in an online way, weighting the individual models proportionally to their recent performance, which allowed the authors to deal with potential non-stationarities in an innovative way. The performance of the models (the predictions of each model) was evaluated by the area under the curve score of the receiver operating characteristic (Kent, 1989) and the proposed model was found to perform better than the benchmark models or equally weighted ensembles. Thirdly, Bucci (2019) studied RV based on monthly S&P returns data (815 observations). Volatility predictors comprised a broad set of macroeconomic and financial variables. This research compared the predictive performance of LSTM to an autoregressive fractionally integrated moving average with the same set of explanatory variables (ARFIMAX) and without determinants (ARFIMA). Results showed that LSTM outperformed traditional econometric methods. Fourthly, Nguyen, Tran, Gunawan & Kohn (2019) introduced the LSTM-SV model, which used LSTM to model the long-term memory and non-linear auto-dependence in volatility dynamics beyond the basic SV model. This research studied weekly stock return and daily exchange rate return time series and showed that the LSTM-SV model efficiently captured non-linear and long memory effects in stock and exchange rate returns and provided better out-of-sample forecasts than a standard SV model (Taylor (1982)) and a non-linearity N-SV model (Yu, Yang & Zhang (2006)). Fifthly, Kim & Kim (2019) proposed the feature fusion long short-term memory convolutional neural network (LSTM-CNN), which combined features learned from different representations of the same data, namely prices time series and chart images of the same data. This research is probably closest to my research in terms of the data used for the study, which was a dataset of HF 1-minute equity ETF data (~97,000 observations). Single models for CNN and LSTM respectively were applied.  The research showed that LSTM-CNN outperformed the single LSTM and CNN models in stock price prediction.  Kim & Kim also showed that a candlestick chart provided the best stock chart image for forecasting stock prices.  These authors research showed that prediction error is efficiently reduced by using a combination of temporal and image features from the same data, rather than using these features separately. Finally, Fischer & Krauss (2017) deployed LSTM networks to predict out-of-sample directional movements on the S&P 500, modeling daily returns for S&P constituents and comparing LSTM to memory free classification methods, including a random forest, a NN, and a logistic regression classifier. This research showed that LSTM networks outperformed the memory free classification methods.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I conclude with a summary of my main insights and contributions. My research extends the multifractal literature in three ways.  Firstly, I compare a deep learning model to a multifractal model, theoretically and empirically, and show that the deep learning and multifractal approaches have many similarities. The forward pass and backward propagation algorithms of the LSTM analogize to the transition matrix and Bayesian recursion of the $\\mathrm{MSM} \\left( \\bar k \\right)$, whilst the LSTM gradient descent training procedure analogizes to the basin-hopping optimization framework in my implementation of $\\mathrm{MSM} \\left( \\bar k \\right)$. Secondly, I provide a thorough derivation of the LSTM network, based upon the framework set out by Sherstinsky (2018, 2020), a step generally skipped in finance deep learning literature featuring the LSTM, to a level of detail that enables (re-)construction of all main algorithms of a LSTM network cell without recourse to an opensource deep learning framework. I thus provide a bridge finance DL literature and the denser theoretical treatment of the LSTM that is available in broader DL literature. Thirdly, I compare both models within the setting of big data, using a very large HF time series that has been thoroughly cleansed and prepared, has suppressed microstructure noise, and features extreme volatility regimes.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "These insights and contributions provide a structured framework for further comparisons of data-centric methods from the machine learning canon to multifractal methods, with an obvious candidate for further research being multivariate settings, predicting relations expected to be nonlinear, such as between realized volatility and its determinants.\n",
    "$\\\\[0.1in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y. & Zheng, X. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. http://tensorflow.org/\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ahmed, A., Smola, A., Wang, Y., Xing, E.P., Zaheer, M. & Zheng, X. (2018). State space LSTM models with particle MCMC inference. arXiv:1711.11179v1.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Almosova, A. & Andresen, N. (2019). Nonlinear inflation forecasting with recurrent neural networks. Working Paper.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Amari, S. (1967). A theory of adaptive pattern classifiers. IEEE Trans. EC, 16, 3 299–307.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Andersen, T.G., Bollerslev, T. & Diebold, F.X. (2007). Roughing it up: Including jump components in the measurement, modelling, and forecasting of return volatility. Review of Economics and Statistics, 2007, 89(4), 701-720.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Andersen, T.G., Bollerslev, T., Diebold, F.X. & Ebens, H. (2001). The distribution of realized stock return volatility. Journal of Financial Economics 61, 43–76.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Archer, E., Park, I.M., Buesing, L., Cunningham, J. & Paninski, L. (2015). Black box variational inference for state space models. arXiv:1511.07367.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Arneric, J., Poklepovic, T. & Aljinovic, Z. (2014). GARCH based artificial neural networks in forecasting conditional variance of stock returns. Croatian Operational Research Review, 5, 329–343.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Atsalakis, G.S. & Valavanis, K. P. (2009). Surveying stock market forecasting techniques, Part II: Soft computing methods. Expert Systems with Applications, 36 (3), 5932-5941.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Balkin, S.D. & Ord, J.K. (2000). Automatic neural network modeling for univariate time series. International Journal of Forecasting, 16(4), 509-515.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bandara, K., Bergmeir, C. & Smyl, S. (2017). Forecasting across time series databases using recurrent neural networks on groups of similar series:\n",
    "A clustering approach. arXiv:1710.03222.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bang-Jensen, J. (2008). Digraphs: Theory, Algorithms and Applications, Springer Monographs. In Mathematics (2nd ed.), Springer-Verlag, 32–34.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Barndorff-Nielsen, O.E., Shephard, N., 2002. Econometric analysis of realised volatility and its use in estimating stochastic volatility models. Journal of Royal Statistical Society, 64, 253–280.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bayer, J. & Osendorfer, C. (2014). Learning stochastic recurrent networks. arXiv:1411.7610.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. (2007). Greedy layer-wise training of deep networks. Advances in Neural Information Processing Systems (NIPS’06), 153-160.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2), 157–166.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bishop, C.M. (1994). Mixture density networks. Technical report, 1994.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Borovkova, S. & Tsiamas, I. (2018). An ensemble of LSTM neural networks for high frequency stock market classification. Journal of Forecasting, 38, 600-619.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Boulanger-Lewandowski, N., Bengio, Y. & Vincent, P. (2012). Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In ICML, 2012.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Butcher, J.C. (2003). Numerical methods for ordinary differential equations. John Wiley & Sons, New York.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bryson, A. E. (1961). A gradient method for optimizing multi-stage allocation processes. In Proc. Harvard Univ. Symposium on digital computers and their applications.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bryson, Jr., A. E. & Denham, W. F. (1961). A steepest-ascent method for solving optimum programming problems. Technical Report BR-1303, Raytheon Company, Missle and Space Division.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bryson, A. & Ho, Y. (1969). Applied optimal control: optimization, estimation, and control. Blaisdell Pub. Co.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bucci, A. (2019). Realized volatility forecasting with neural networks. MPRA Paper No. 95443. Online at https://mpra.ub.uni-muenchen.de/95443/.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Caley, J.A., Lawrance, N.R.J. & Hollinger, G.A. (2017). Deep networks with confidence bounds for robotic information gathering.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Calvet, L.E. & Fisher, A.J. (2001). Forecasting multifractal volatility, Journal of Econometrics 105, 27-58.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Calvet, L.E. & Fisher, A.J. (2002). Multifractality in asset returns: Theory and Evidence. The Review of Economics and Statistics, Vol. LXXXIV, No. 3, 381-406\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Calvet, L.E. & Fisher, A.J. (2004). How to forecast long-run volatility: Regime switching and the estimation of multifractal processes. Journal of Financial Econometrics 2 (1), 49-83.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Calvet, L.E. & Fisher, A.J. (2007). Multi-frequency news and stock returns. Journal of Financial Economics 86 (1), 178-212.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Calvet, L.E. & Fisher, A.J. (2008). Multifractal Volatility Theory, Forecasting, and Pricing. Elsevier, Academic Press.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Calvet, L.E. & Fisher, A.J. (2013). Extreme Risk and Fractal Regularity in Finance. In Carfì, D., Lapidus, M.L., Pearse, E.P.J. & van Frankenhuijsen, M. (Eds.), Fractal Geometry and Dynamical Systems in Pure and Applied Mathematics II: Fractals in Applied Mathematics, Contemporary Mathematics,\n",
    "Volume 601.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Carr, P., Wu, L. & Zhang, Z.B. (2020). Using machine learning to predict realized variance. Journal of Investment Management, Vol. 18, No. 2, 1-16.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Chang, C.H., Rampasek, L. & Goldenberg, A. (2017). Dropout feature ranking for deep learning models. arXiv:1712.08645.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Che, Z.P., Purushotham, S., Cho, K.Y., Sontag, D., & Liu, Y. (2018). Recurrent neural networks for multivariate time series. Scientific reports, 8(1), 6085.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Cho, K.H., Chung, J.Y., Gulcehre, C. & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Chollet, F. (2015). Keras. https://keras.io.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Chollet, F. (2018). Deep learning with Python. Manning, Shelter Island.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Chung, J.Y., Kastner, K., Dinh, L., Goel, K., Courville, A. & Bengio, Y. (2015). A recurrent latent variable model for sequential data. In NIPS, 2015.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Cinar, Y.G., Mirisaee, H., Goswami, P., Gaussier, E, Ait-Bachir, A. & Strijov, F.V. (2017). Time series forecasting using RNNs: an extended attention mechanism to model periods and handle missing values. CoRR, abs/1703.10089.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Collins, J.L. (2020). Multifractal volatility predictions with a high-dimensional state space using high frequency data with suppressed microstructure noise. PhD Thesis Paper 1.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Connor, J.T., Douglas Martin, R. & Atlas, L.E. (1994). Recurrent neural networks and robust time series prediction. IEEE transactions on neural networks, Vol. 5, No. 2, 240-254.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "de Vries, B. & Principe, J.C. (1991). A theory for neural networks with time delays. In Lippmann, R.P., Moody, J.E. & Touretzky, D.S. (Eds), Advances in Neural Information Processing Systems 3, Morgan Kaufmann. 162–168.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Diebold, F.X., Ghysels, E., Mykland, P. & Zhang, L. (2019). Big data in dynamic predictive econometric modeling. Editorial in Journal of Econometrics, 212, 1-3.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Director, S. W. & Rohrer, R. A. (1969). Automated network design - the frequency-domain case. IEEE Trans. Circuit Theory, CT, 16, 330–337.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Dixon, M., Klabjan, D. & Bang, J.H. (2015). Implementing deep neural networks for fnancial market prediction on the Intel Xeon Phi. In: roceedings of the 8th Workshop on High Performance Computational Finance. 1-6.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Dixon, M., Klabjan, D. & Bang, J.H. (2017). Classification-based financial markets prediction using deep neural networks. Algorithmic Finance, 6, 67–77\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Donaldson, G.R. & Kamstra, M. (1996a). A new dividend forecasting procedure that rejects bubbles in asset prices. Review of Financial Studies, 8, 333–383.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Donaldson, G.R. & Kamstra, M. (1996b). Forecast Combining with Neural Networks. Journal of Forecasting, 15, 49–61.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Donaldson, G.R. & Kamstra, M. (1997). An artificial neural network-GARCH model for international stock return volatility. Journal of Empirical Finance 4, 17–46.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Downey, C., Hefny, A. & Gordon, G. (2017). Practical learning of predictive state representations. arXiv preprint arXiv:1702.04121.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Dreyfus, S. E. (1962). The numerical solution of variational problems. Journal of Mathematical Analysis and Applications, 5, 30–45.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Faraway, J. & Chatfield, C. (1998). Time series forecasting with neural networks: a comparative study using the air line data. Journal of the Royal Statistical Society: Series C (Applied Statistics), 47(2), 231-250.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Fernandes, M., Medeiros, M.C., Scharth, M., 2014. Modeling and predicting the CBOE market volatility index. Journal of Banking & Finance, 40, 1–10.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Filimonov, V. & Sornette, D. (2011). Self-excited multifractal dynamics. arXiv:1008.1430v2 [cond-mat.stat-mech] 19 Apr 2011.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Fischer, T. & Krauss, C. (2017). Deep learning with long short term memory networks for financial market predictions. FAU Discussion Papers in Economics, No. 11/2017, Friedrich-Alexander-Universität Erlangen-Nürnberg, Institute for Economics, Erlangen.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Flunkert, V., Salinas, D. & Gasthaus, J. (2017). Deepar: Probabilistic forecasting with autoregressive recurrent networks. arXiv preprint arXiv:1704.04110, 2017.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Fraccaro, M., Sønderby, S.K., Paquet, U. & Winther, O. (2016). Sequential neural models with stochastic layers. In NIPS, 2016.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Garman, M. B. & Klass, M. J. (1980). On the estimation of security price volatilities from historical data. Journal of Business, 53(1), 67–78.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H. & Schmidhuber, J. (2009). A Novel connectionist system for improved unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31, 5.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gauss, C. F. (1809). Theoria motus corporum coelestium in sectionibus conicis solem ambientium.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gauss, C. F. (1821). Theoria combinationis observationum erroribus minimis obnoxiae (Theory of the combination of observations least subject to error).\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gers, F. A., Schmidhuber, J., & Cummins, F. (2000). Learning to forget: Continual prediction with LSTM. Neural Computation, 12, 2451–2471.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gers, F.A., Schraudolph, N.N. & Schmidhuber, J. (2002). Learning precise timing with LSTM recurrent networks. Journal of machine learning research, 3, 115-143.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ghahramani, Z. & Roweis, S.T. (1999). Learning nonlinear dynamical systems using an EM algorithm. In NIPS, 1999.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Giles, C.L., Lawrence, S. & Tsoi, A.C. (2001). Noisy time series prediction using recurrent neural networks and grammatical inference. Machine learning 44, 1, 161-183.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Giles, C.L., Sun, G.Z., Chen, H.H., Lee, Y.C. & Chen, D. (1989). Higher order recurrent networks and grammatical inference. NIPS, 380-387.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gisslen, L., Luciw, M., Graziano, V. & Schmidhuber, J. (2011). Sequential constant size compressor for reinforcement learning. In Proc. Fourth Conference on Artificial General Intelligence (AGI), Google, Mountain View, CA, 31–40. Springer.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gomez, F., Schmidhuber, J. & Miikkulainen, R. (2008). Accelerated neural evolution through cooperatively coevolved synapses. Journal of Machine Learning Research, 9, 937-965.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Goodfellow, I., Bengio, Y. & Courville, A. (2016). Deep Learning, MIT Press.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Graves, A. (2014). Generating sequences with recurrent neural networks. arXiv:1308.0850v5.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Graves, A., Mohamed, A. & Hinton, G. (2013). Speech recognition with deep recurrent neural networks. arXiv:1303.5778.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Graves, A. & Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks 18:5-6, 602-610.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Greff, K., Srivastava, R.K., Koutnik, J., Steunebrink, B.R. & Schmidhuber, J. (2017). LSTM: A search space odyssey. IEEE transactions on neural\n",
    "networks and learning systems, 28(10), 2222-2232.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gregor, K., Danihelka, I, Graves, A., Rezende, D.J. & Wierstra, D. (2015). DRAW: A recurrent neural network For image generation. In ICML, 2015.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Grossberg, S. (1969). Some networks that can learn, remember, and reproduce any number of complicated space-time patterns, I. Journal of Mathematics and Mechanics, 19, 53–91.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Grossberg, S. (1988). onlinear neural networks: Principles, mechanisms, and architectures. Neural Networks, 1, 17–61.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Grossberg, S. (2013). Recurrent neural networks. Scholarpedia, 8(2):1888.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gu, S.H., Kelly, B.T. & Xiu, D.C (2020). Empirical asset pricing via machine learning. The Review of Financial Studies, Volume 33, Issue 5, 2223–2273.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gu, S.H., Kelly, B.T. & Xiu, D.C (2019). Autoencoder asset pricing models. Yale ICF Working Paper No. 2019-04; Chicago Booth Research Paper No. 19-24.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Guo, T., Xu, Z., Yao, X, Chen, H.F., Aberer, K. & Funaya, K. (2016). Robust online time series prediction with recurrent neural networks. In Data\n",
    "Science and Advanced Analytics (DSAA), 2016 IEEE International Conference, 816-825. Ieee, 2016.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hajizadeh, E., Seifi, A., Fazel Zarandi, M. & Turksen, I. (2012). A hybrid modeling approach for forecasting the volatility of S&P 500 index return. Expert Systems with Applications 39, 431–436.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hamid, S.A. & Iqbal, Z. (2004). Using neural networks for forecasting volatility of S&P 500 index futures prices. Journal of Business Research, 57, 1116-1125. \n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Harris, C.R., Millman, K.J., van der Walt, S.J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N.J., Kern, R., Picus, M., Hoyer, S., Brett, M., Haldane, A., Fernandez del Rio, A., Wiebe, M., Sheppard, K., Reddy, T., Weckesser, W. & Oliphant, T.E. (2020). Array programming with NumPy. Nature 585, 357–362.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hastie, T., Tibshirani, R. & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. 2Ed, Springer.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Heaton, J.B., Polson, N.G. & Witte, J.H. (2016). Deep learning in finance, arXiv:1602.06561.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hebb, D. O. (1949). The Organization of Behavior. Wiley, New York.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hinton, G.E., Osindero, S. & Teh, Y.W. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18 (7).\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hinton, G.E. & Salakhutdinov, R. (2006). Reducing the dimensionality of data with neural networks. Science, 313(5786), 504–507.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R.R. (2012). Improving neural networks by preventing co-adaptation\n",
    "of feature detectors. arXiv:1207.0580.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hochreiter, S. & Schmidhuber, J. (1997). Long short-term memory. Neuralcomputation, 9(8): 1735-1780.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hochreiter, S., Bengio, Y., Paolo, F. & Schmidhuber, J. (2001). Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. In Kremer & Kolen (Eds), A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hornik, K., Stinchcombe, M. & White, H. (1989). Multilayer feedforward networks are universal approximators. Neural networks 2, 359-366.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hornik, K. (1991). Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2), 251–257.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hsu, D. (2017). Time series forecasting based on augmented long short-term memory. arXiv:1707.00666.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hu, Y.M. & Tsoukalas, C. (1999). Combining conditional volatility forecasts using neural networks: an application to the EMS exchange rates. Journal of International Financial Markets, Institutions & Money 9, 407–422.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Huck, N. (2009). Pairs selection and outranking: An application to the S&P 100 index. European Journal of Operational Research 196, 2, 819-825.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Huck, N. (2010). Pairs trading and outranking: The multi-step-ahead forecasting case. European Journal of Operational Research 207, 3, 1702-1716.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Huffman, D. A. (1952). A method for construction of minimum-redundancy codes. Proceedings IRE, 40, 1098–1101.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ioffe, S. & Szegedy, C. (2015). Batch normalization: accelerating deep network training by reducing internal covariate shift”. arXiv:1502.03167.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ivakhnenko, A. G. (1968). The group method of data handling – a rival of the method of stochastic approximation. Soviet Automatic Control, 13, 3, 43–55.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ivakhnenko, A. G. (1971). Polynomial theory of complex systems. IEEE Transactions on Systems, Man and Cybernetics, 4, 364–378.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ivakhnenko, A. G. & Lapa, V. G. (1965). Cybernetic Predicting Devices. CCM Information Corporation.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ivakhnenko, A. G., Lapa, V. G., & McDonough, R. N. (1967). Cybernetics and forecasting techniques.  American Elsevier, NY.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Israel, R., Kelly, B.T. & Moskowitz, T. (2020). Can machines \"learn\" finance? Journal of Investment Management, Volume 18, No. 2.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Johnson, M.J., Duvenaud, D., Wiltschko, A.B., Datta, S.R. & Adams, R.P. (2016). Composing graphical models with neural networks for structured representations and fast inference. In NIPS, 2016.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Julier, S.J. & Uhlmann, J.K. (1997). A new extension of the Kalman filter to nonlinear systems. In International symposium on aerospace/defense sensing, simulation and controls 1.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Kamijo, K. & Tanigawa, T. (1990). Stock price pattern recognition - a recurrent neural network approach. 1990 IJCNN International Joint Conference on Neural Networks.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Karl, M., Soelch, M., Bayer, J. & van der Smagt, P. (2017). Deep variational Bayes filters: Unsupervised learning of state space models from raw data. In ICLR, 2017.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Karpathy, A., Johnson, J. & Li, F.F. (2015). Visualizing and understanding recurrent networks. arXiv:1506.02078.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Khan, A.I. (2011). Financial volatility forecasting by nonlinear support vector machine heterogeneous autoregressive model: Evidence from Nikkei 225 Stock Index. International Journal of Economics and Finance 3, 138–150.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Kingma, D.P. & Welling, M. (2014). Auto-Encoding Variational Bayes. In ICLR, 2014.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Kelley, H. J. (1960). Gradient theory of optimal flight paths. ARS Journal, 30, 10, 947–954.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Kim, T. & Kim, H.Y. (2019). Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data. PLoS ONE 14(2): e0212320.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Kohonen, T. (1988). Self-Organization and Associative Memory. Springer, second edition.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "kornblith, S., Norouzi, M., Lee, H. & Hinton, G.E. (2019). Similarity of neural network representations revisited. Proceedings of the 36th International Conference on Machine Learning, Long Beach, California.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Krishnan, R.G., Shalit, U. & Sontag, D. (2015). Deep Kalman Filters. arXiv:1511.05121.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Krishnan, R.G., Shalit, U. & Sontag, D. (2017). Structured inference networks for nonlinear state space models. In AAAI, 2017.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Kyrychko, Y. & Hogan, S. (2010). On the use of delay equations in engineering applications. 16, 943–960.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Laptev, N., Yosinski, J., Li, L.E. & Smyl, S. (2017). Time-series extreme event forecasting with neural networks at Uber. In International\n",
    "Conference on Machine Learning, 34, pages 1-5.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "LeCun, Y. (1985). Une proc´edure d’apprentissage pour r´eseau `a seuil asym´etrique. Proceedings of Cognitiva 85, Paris, 599–604.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "LeCun, Y. (1988). A theoretical framework for back-propagation. In Touretzky, D., Hinton, G., & Sejnowski, T., editors, Proceedings of the 1988 Connectionist Models Summer School, 21–28, CMU, Pittsburgh, Pa. Morgan Kaufmann.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "LeCun, Y., Bengio, Y. & Hinton, G. (2015). Deep learning. Nature, 521(7553): 436-444.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Legendre, A. M. (1805). Nouvelles m´ethodes pour la d´etermination des orbites des cometes. F. Didot.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Lipton, Z., Berkowitz, J. & Elkan, C. (2015). A critical review of recurrent neural networks for sequence learning. arXiv:1506.00019.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Liu, F., Pantelous, A.A. & von Mettenheim, H.J. (2018). Forecasting and trading high frequency volatility on large indices. Quantitative Finance, Vol. 18, 5, 737-748\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Luo, R., Zhang, W.N., Xu, X.J. & Wang, J. (2018). A neural stochastic volatility model. The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), 6401-6408.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Lux, T. (2018). Inference for nonlinear state space models: A comparison of different methods applied to Markov-switching multifractal models. CAU, Economics Working Paper, No. 2018-07.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Maciel, L., Gomide, F. & Ballini, R. (2016). Evolving Fuzzy-GARCH approach for financial volatility modeling and forecasting. Computational Economics 48, 379–398.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Malhotra, P., Vig, L, Shroff, G. & Agarwal, P. (2015). Long short term memory networks for anomaly detection in time series. In Proceedings, 89. Presses universitaires de Louvain, 2015.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Malliaris, M. & Salchenberger, L. (1996). Using neural networks to forecast the S&P 100 implied volatility. Neurocomputing, 10(2), 183–195.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Mayer, H., Gomez, F., Wierstra, D., Nagy, I., Knoll, A. & Schmidhuber, J. (2008). A system for robotic heart surgery that learns to tie knots using recurrent neural networks. Advanced Robotics, 22/13-14, 1521-1537.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "McCulloch, W. & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 7, 115–133.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A. & Teller, E. (1953). Equation of state calculations by fast computing machines. Journal of Chemical Physics, 21, 1087.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A. & Teller, E. (1953). Equation of state calculations by fast computing machines. Journal of Chemical Physics, 21, 1087.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Minsky, M.L. & Papert, S.A. (1990). Perceptrons, 2nd Ed. MIT Press, Cambridge MA.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Moritz, B. & Zimmermann, T. (2014). Deep conditional portfolio sorts: The relation between past and future stock returns. Working paper, LMU Munich and Harvard University.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Namin, S.S. & Namin, S.N. (2018). Forecasting economic and financial time series: ARIMA vs. LSTM. Working Paper.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Narendra, K. S. & Thathatchar, M. A. L. (1974). Learning automata – a survey. IEEE Transactions on Systems, Man, and Cybernetics, 4, 323–334.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Nguyen, N., Tran, M.N., Gunawan, D. & Kohn, R. (2019). A long short-term memory stochastic volatility model. arXiv:1906.02884v2 [\n",
    "econ.EM] 30 Sep 2019.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Parker, D. B. (1985). Learning-logic. Technical Report TR-47, Center for Comp. Research in Economics and Management Sci., MIT.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Perez-Ortiz, J. A., Gers, F. A., Eck, D., & Schmidhuber, J. (2003). Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets. Neural Networks, 16, 241–250.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Pachitariu, M. & Sahani, M. (2012). Learning visual motion in recurrent neural networks. In NIPS, 2012.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Parkinson, M. (1980). The extreme value method for estimating the variance of the rate of return. Journal of Business, 53(1), 61–65.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Pascanu, R., Mikolov, T. & Bengio, Y. (2013). On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, 1310–1318.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J. & Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Wallach, H., Larochelle, H., Beygelzimer, A., Fox, E. & Garnett, R. (Eds.), Advances in Neural Information Processing Systems 32, Curran Associates, Inc. https://pytorch.org/ \n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Patel, A.B., Nguyen, T. & Baraniuk, R.G. (2015). A probabilistic theory of deep learning. Rice University Electrical and Computer Engineering Dept. Technical Report No 2015-1. arXiv:1504.00641\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Petnehazi, G. & Gall, J. (2019). Exploring the predictability of range‐based volatility estimators using recurrent neural networks. Intell Sys Acc Fin Mgmt. 2019; 1–8.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Petnehazi, G. (2019). Recurrent neural networks for time series forecasting. arXiv:1901.00069v1 [cs.LG].\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Pontryagin, L. S., Boltyanskii, V. G., Gamrelidze, R. V., & Mishchenko, E. F. (1961). The Mathematical Theory of Optimal Processes.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Rangapuram, S.S., Seeger, M., Gasthaus, J., Stella, L., Wang, Y.Y. & Januschowski, T. (2018). Deep state space models for time series forecasting. 32nd Conference on Neural Information Processing Systems (NeurIPS 2018).\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Rezende, D.J., Mohamed, S. & Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Richard, J.P. (2003). Time delay systems: An overview of some recent advances and open problems. Automatica, 39, 10, 1667–1694.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Rogers, L. C. G. & Satchell, S. E. (1991). Estimating variance from high, low and closing prices. The Annals of Applied Probability, 1(4), 504–512.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Roh, T. H. (2007). Forecasting the volatility of stock price index. Expert Systems with Applications, 33(4), 916–922.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6), 386.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Rosenblatt, F. (1962). Principles of Neurodynamics. Spartan, New York.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Russell, S. J., Norvig, P., Canny, J. F., Malik, J. M., & Edwards, D. D. (1995). Artificial Intelligence: a Modern Approach, Vol 2. Englewood Cliffs: Prentice Hall.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Schmidhuber, J. (1992). Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2), 234–242.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Schmidhuber, J., Wierstra, D., Gagliolo, M. & Gomez, F. (2007). Training recurrent networks by Evolino. Neural Computation, 19(3), 757-779.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Schmidhuber, J. (2013). My first Deep Learning system of 1991 + Deep Learning timeline 1962-2013. Technical Report arXiv:1312.5548v1 [cs.NE], The Swiss AI Lab IDSIA.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural networks, 61, 85–117.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sermpinis, G., Theoflatos, K., Karathanasopoulos, A., Georgopoulos, E.F. & Dunis, C. (2013). Forecasting foreign exchange rates with adaptive neural networks using radial-basis functions and particle swarm optimization. European Journal of Operational Research 225, 3, 528-540.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sezer, O.B., Gudelek, M.U. & Ozbayoglu, A.M. (2019). Financial time series forecasting with deep learning: a systematic literature review 2005-2019. arXiv:1911.13288v1\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Shannon, C. E. (1948). A mathematical theory of communication (parts I and II). Bell System Technical Journal, XXVII, 379–423.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sherstinsky, A. & Picard, R.W. (1998). On stability and equilibria of the M-Lattice. IEEE Transactions on Circuits and Systems – I: Fundamental Theory and Applications, 45, 4, 408–415.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sherstinsky, A. (2018a). Deriving the Recurrent Neural Network Definition and RNN Unrolling Using Signal Processing. In Critiquing and Correcting Trends in Machine Learning Workshop, NeurIPS 31.\n",
    "2018), Dec 2018.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sherstinsky, A. (2018b). Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network, arxiv:1808.03314.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sherstinsky, A. (2020). Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Physica D: Nonlinear Phenomena, Volume 404, March 2020: Special Issue on Machine Learning and Dynamical Systems.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Siah, K. W. & Myers, P. (2016). Stock market prediction through technical and public sentiment analysis. Available online at http://kienwei.mit.edu/sites/default/files/images/stock-market-prediction.pdf\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Siegelmann, H. T. & Sontag, E. D. (1991). Turing computability with neural nets. Applied Mathematics Letters, 4(6), 77–80.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Soltani, R. & Jiang, H. (2016). Higher order recurrent neural networks. arXiv:1605.00064.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from over-fitting. Journal of Machine Learning Research, 15(1), 1929-1958.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sutskever, I., Martens, J. & Hinton, G.E. (2011). Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 1017-1024.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sutton, R.S. & Barto, A.G. (2018). Reinforcement learning, an introduction. 2nd Ed. The MIT Press, Cambridge, MA.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Tang, Z.Y., De Almeida, C. & Fishwick, P.A. (1991). Time series forecasting using neural networks vs. box-jenkins methodology. Simulation, 57(5), 303-310, 1991.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Takeuchi, L. & Lee, Y.Y. (2013). Applying deep learning to enhance momentum trading strategies in stocks. Working paper, Stanford University.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Timmermann, A. (2018). Forecasting methods in finance. Annual Review of Financial Economics, Vol. 10, 449-479.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Verstyuk, S. (2019). Modeling multivariate time series in economics: from auto-regressions to recurrent neural networks. Unpublished paper.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Van der Westhuizen, J. &  Lasenby, J. (2017). Visualizing LSTM decisions. Stat, 1050:23.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Von der Malsburg, C. (1973). Self-organization of orientation sensitive cells in the striate cortex. Kybernetik, 14(2), 85–100.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Vortelinos, D.I., 2017. Forecasting realized volatility: HAR against Principal Components Combining, neural networks and GARCH. Research in International Business and Finance, 39, 824–839.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Werbos, P.J (1974). Beyond regression: new tools for prediction and analysis in the behavioral sciences. Ph. D. dissertation, Harvard University.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Werbos, P. J. (1981). Applications of advances in nonlinear sensitivity analysis. In Proceedings of the 10th IFIP Conference, 31.8 - 4.9, NYC, 762–770.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Werbos, P.J. (1988). Generalization of backpropagation with application to a recurrent gas market model. Neural networks, 1(4), 339-356.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Werbos, P.J. (1990). Backpropagation through time: what does it do and how to do it. In Proceedings of IEEE, volume 78, 1550–1560.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "White, H. (1988). Economic prediction using neural networks: the case of IBM daily stock returns. IEEE 1988 International Conference on Neural Networks.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Widrow, B. & Hoff, M. (1962). Associative storage and retrieval of digital information in networks of adaptive neurons. Biological Prototypes and Synthetic Systems, 1-160.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Wilkinson, J. H., editor (1965). The Algebraic Eigenvalue Problem. Oxford University Press, Inc., New York, NY, USA.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Williams, R. J. (1989). Complexity of exact gradient computation algorithms for recurrent neural networks. Technical Report Technical Report NU-CCS-89-27, Boston: Northeastern University, College of Computer Science.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Willshaw, D. J. & von der Malsburg, C. (1976). How patterned neural connections can be set up by self-organization. Proc. R. Soc. London B, 194, 431–445.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Xiong, R.X., Nichols, E.P. & Shen, Y. (2016). Deep learning stock volatility with Google domestic trends. arXiv:1512.04916v3.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Yang, D. & Zhang, Q. (2000). Drift‐independent volatility estimation based on high, low, open, and close prices. The Journal of Business, 73(3),\n",
    "477–492.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Yu, R., Zheng, S. Anandkumar, A. & Yue, Y. (2019). Long-term forecasting using higher-order tensor RNNs. Journal of Machine Learning Research 1 (2019) 1-48.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Yu, J., Yang, Z. & Zhang, X. (2006). A class of nonlinear stochastic volatility models and its implications for pricing currency options. Computational Statistics and Data Analysis, 51(4), 2218-2231.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Zaheer, M., Ahmed, A. & Smola, A.J. (2017). Latent LSTM allocation: Joint clustering and non-linear dynamic modeling of sequence data. International Conference on Machine Learning, Sydney, Australia, PMLR 70.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Zhang, P.G. & Min, Q. (2005). Neural network forecasting for seasonal and trend time series. European journal of operational research, 160(2), 501-514.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Zheng, S., Yue, Y.S., & Lucey, P. (2016). Generating long-term trajectories using deep hierarchical networks. In Advances in Neural Information Processing Systems, 1543-1551.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Zhu, L.X. & Laptev, N. (2017). Deep and confident prediction for time series at Uber. In Data Mining Workshops (ICDMW), 2017 IEEE International\n",
    "Conference, 103-110. IEEE, 2017.\n",
    "$\\\\[0.05in]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
